{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "from typing import Callable, List, Optional\n",
    "from torch.library import Library\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def diagonal_op_(input, offset: int=0, op: Optional[Callable]=None):\n",
    "    crow_indices = input.crow_indices() # b + 1 dimensional\n",
    "    col_indices = input.col_indices() # b + 1 dimensional\n",
    "    bsr_values = input.values() # 1 + 2 dimensional\n",
    "    m, n = input.shape[-2], input.shape[-1]\n",
    "    dm, dn = (bsr_values.shape[-2], bsr_values.shape[-1])\n",
    "    sm, sn = m // dm, n // dn\n",
    "\n",
    "    #simple case(block is square and offset is 0)\n",
    "    if dm == dn and offset == 0:\n",
    "        dummy_val = torch.zeros(bsr_values.shape[0], device='cpu')\n",
    "        dummy = torch.sparse_csr_tensor(crow_indices=crow_indices.to('cpu'),\n",
    "                                        col_indices=col_indices.to('cpu'),\n",
    "                                        values=dummy_val)\n",
    "        dummy_coo = dummy.to_sparse(layout=torch.sparse_coo).coalesce()\n",
    "\n",
    "        indices = dummy_coo.indices().to(input.device)\n",
    "        diag_indices = (indices[0] == indices[1]).nonzero().squeeze(-1)\n",
    "        block_diags = bsr_values.diagonal(dim1=-2, dim2=-1)\n",
    "        values = block_diags[diag_indices]\n",
    "        n_diag_blocks = sm if sm < sn else sn\n",
    "        if diag_indices.shape[-1] == n_diag_blocks:\n",
    "            results = values\n",
    "        else:\n",
    "            results_shape = (n_diag_blocks, dm)\n",
    "            results = torch.zeros(results_shape, dtype=values.dtype, device=values.device)\n",
    "            results[indices[0, diag_indices]] = values\n",
    "            assert op is None, \"op is not supported for diagonal that has empty values.\"\n",
    "        results = torch.flatten(results, start_dim=-2, end_dim=-1)\n",
    "        # apply the inplace op\n",
    "        if op is not None:\n",
    "            results = op(results)\n",
    "            block_diags[diag_indices] = results.view(n_diag_blocks, dm)\n",
    "        return results\n",
    "    else:\n",
    "        raise NotImplementedError('Only square block and offset 0 is supported.')\n",
    "\n",
    "\n",
    "def _sparse_csr_mm(mat1, mat2):\n",
    "    if isinstance(mat1, torch.Tensor) and mat1.layout == torch.sparse_bsr:\n",
    "        if isinstance(mat2, torch.Tensor) and mat2.layout == torch.sparse_bsc:\n",
    "            return bsr_bsc_matmul(mat1, mat2)\n",
    "    elif isinstance(mat1, torch.Tensor) and mat1.layout == torch.sparse_bsc:\n",
    "        if isinstance(mat2, torch.Tensor) and mat2.layout == torch.sparse_bsr:\n",
    "            raise NotImplemented\n",
    "    #https://github.com/pytorch/pytorch/blob/3fa3ed4923c19a2b8d2da69e994169b4c8ac5fe3/\n",
    "    #aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp#L789\n",
    "    if mat1.is_sparse_csr and mat2.is_sparse_csr:\n",
    "        size = [mat1.size(0), mat2.size(1)]\n",
    "        zero = torch.zeros(size, dtype=mat2.dtype, device=mat2.device, layout=mat2.layout)\n",
    "        return torch.addmm(zero, mat1, mat2, beta=0.0, alpha=1.0)\n",
    "\n",
    "    if (mat1.layout == torch.sparse_csc or mat1.layout == torch.sparse_csr) and\\\n",
    "        (mat2.layout == torch.sparse_csc or mat2.layout == torch.sparse_csr):\n",
    "        return _sparse_csr_mm(mat1.to_sparse_csr(), mat2.to_sparse_csr())\n",
    "\n",
    "    if mat1.layout == torch.sparse_csc and mat2.layout == torch.strided:\n",
    "        return _sparse_csr_mm(mat1.to_sparse_csr(), mat2)\n",
    "\n",
    "    if mat2.layout == torch.strided:\n",
    "        size = [mat1.size(0), mat2.size(1)]\n",
    "        zero = torch.zeros(size, dtype=mat1.dtype, device=mat1.device, layout=mat2.layout)\n",
    "        return torch.addmm(zero, mat1, mat2, beta=0.0, alpha=1.0)\n",
    "\n",
    "    size = [mat1.size(0), mat2.size(1)]\n",
    "    zero = torch.zeros(size, dtype=mat1.dtype, device=mat1.device, layout=mat1.layout),\n",
    "    return torch.addmm(zero, mat1, mat2, beta=0.0, alpha=1.0)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def bsr_bsc_matmul(bsr:torch.Tensor, bsc:torch.Tensor):\n",
    "    assert bsr.shape[-1] == bsc.shape[-2]\n",
    "    assert bsr.layout == torch.sparse_bsr or bsr.layout == torch.sparse_csr\n",
    "    assert bsc.layout == torch.sparse_bsc or bsc.layout == torch.sparse_csc\n",
    "    crow_indices = bsr.crow_indices() # b + 1 dimensional\n",
    "    idx_dtype = crow_indices.dtype\n",
    "    crow_indices: List[int] = crow_indices.tolist()\n",
    "    col_indices: List[int] = bsr.col_indices().tolist() # b + 1 dimensional\n",
    "    csr_values = bsr.values() # 1 + 2 dimensional\n",
    "\n",
    "    ccol_indices: List[int] = bsc.ccol_indices().tolist() # b + 1 dimensional\n",
    "    row_indices: List[int] = bsc.row_indices().tolist() # b + 1 dimensional\n",
    "    csc_values = bsc.values() # 1 + 2 dimensional\n",
    "\n",
    "\n",
    "    assert bsr.ndim == 2 and bsc.ndim == 2, \"bsr and bsc must be 2 dimensional. \\\n",
    "    batch dimension is yet not supported.\"\n",
    "    m, n, p = bsr.shape[-2], bsr.shape[-1], bsc.shape[-1]\n",
    "    dm, dn, dp = csr_values.shape[-2], csr_values.shape[-1], csc_values.shape[-1]\n",
    "    sm, sn, sp = m // dm, n // dn, p // dp\n",
    "    assert dm * sm == m\n",
    "    assert dn * sn == n\n",
    "    assert dp * sp == p\n",
    "\n",
    "    result_step: int = 0\n",
    "    coo_indices: List[int] = list()\n",
    "    index: List[int] = list()\n",
    "    source: List[int] = list()\n",
    "    for i in range(sm):\n",
    "        for j in range(sp):\n",
    "            nz: bool = False\n",
    "            k2 = ccol_indices[j]\n",
    "            for k1 in range(crow_indices[i], crow_indices[i+1]):\n",
    "                if k2 == ccol_indices[j+1]:\n",
    "                    break\n",
    "                while row_indices[k2] < col_indices[k1] and k2 < ccol_indices[j+1] - 1:\n",
    "                    k2 += 1\n",
    "                if row_indices[k2] == col_indices[k1]:\n",
    "                    index.append(result_step)\n",
    "                    source.append(k1)\n",
    "                    source.append(k2)\n",
    "                    nz = True\n",
    "            if nz:\n",
    "                result_step += 1\n",
    "                coo_indices.append(i)\n",
    "                coo_indices.append(j)\n",
    "    source = torch.tensor(source, dtype=idx_dtype, device=bsr.device).view(-1, 2)\n",
    "    index = torch.tensor(index, dtype=idx_dtype, device=bsr.device)\n",
    "    prod = torch.bmm(csr_values[source[:, 0]], csc_values[source[:, 1]])\n",
    "    values_shape = (result_step, dm, dp)\n",
    "    reduced = torch.zeros(values_shape, dtype=prod.dtype, device=prod.device)\n",
    "    reduced.scatter_add_(0, index.unsqueeze(-1).unsqueeze(-1).expand_as(prod), prod)\n",
    "    # Indices should be prepared on CPU, IN ANY CASE\n",
    "    coo_indices = torch.tensor(coo_indices, dtype=idx_dtype, device='cpu')\n",
    "    coo_indices = coo_indices.view(-1, 2).T\n",
    "    # use fake coo\n",
    "    dummy_val = torch.zeros(coo_indices.shape[-1], dtype=prod.dtype, device='cpu')\n",
    "    dummy = torch.sparse_coo_tensor(indices=coo_indices, values=dummy_val, size=(sm, sp))\n",
    "    dummy_csr = dummy.coalesce().to_sparse_csr()\n",
    "    crow = dummy_csr.crow_indices().to(bsr.device)\n",
    "    col = dummy_csr.col_indices().to(bsr.device)\n",
    "    return torch.sparse_bsr_tensor(crow, col, reduced, size=(m, p), dtype=reduced.dtype, device=reduced.device)\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    sparse_lib = Library('aten', 'IMPL')\n",
    "    sparse_lib.impl('mm', _sparse_csr_mm, 'SparseCsrCPU')\n",
    "    sparse_lib.impl('mm', _sparse_csr_mm, 'SparseCsrCUDA')\n",
    "    sparse_lib.impl('diagonal', diagonal_op_, 'SparseCsrCPU')\n",
    "    sparse_lib.impl('diagonal', diagonal_op_, 'SparseCsrCUDA')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5RoSZ7kJg-W3"
   },
   "source": [
    "# Bundle Adjustment Example using SparsePyBA and the BAL dataset\n",
    "\n",
    "```\n",
    "The dataset is from the following paper:  \n",
    "Sameer Agarwal, Noah Snavely, Steven M. Seitz, and Richard Szeliski.  \n",
    "Bundle adjustment in the large.  \n",
    "In European Conference on Computer Vision (ECCV), 2010.  \n",
    "```\n",
    "\n",
    "Link to the dataset: https://grail.cs.washington.edu/projects/bal/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data for ladybug...\n",
      "Fetched problem-49-7776-pre from ladybug\n"
     ]
    }
   ],
   "source": [
    "from bal_loader import build_pipeline\n",
    "\n",
    "TARGET_DATASET = \"ladybug\"\n",
    "TARGET_PROBLEM = \"problem-49-7776-pre\"\n",
    "MAX_PIXELS = 31843\n",
    "\n",
    "# TARGET_DATASET = \"trafalgar\"\n",
    "# TARGET_PROBLEM = \"problem-257-65132-pre\"\n",
    "# MAX_PIXELS = 225911\n",
    "\n",
    "DEVICE = 'cpu' # change device to CPU if needed\n",
    "\n",
    "def filter_problem(x):\n",
    "  return x['problem_name'] == TARGET_PROBLEM\n",
    "\n",
    "dataset_pipeline = build_pipeline(dataset=TARGET_DATASET, cache_dir='bal_data').filter(filter_problem)\n",
    "dataset_iterator = iter(dataset_pipeline)\n",
    "dataset = next(dataset_iterator)\n",
    "\n",
    "print(f'Fetched {TARGET_PROBLEM} from {TARGET_DATASET}')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pypose as pp\n",
    "\n",
    "def trim_dataset(dataset, max_pixels):\n",
    "  trimmed_dataset = dict()\n",
    "  trimmed_dataset['points_2d'] = dataset['points_2d'][:max_pixels]\n",
    "  trimmed_dataset['point_index_of_observations'] = dataset['point_index_of_observations'][:max_pixels]\n",
    "  trimmed_dataset['camera_index_of_observations'] = dataset['camera_index_of_observations'][:max_pixels]\n",
    "  # other fields are not changed\n",
    "  trimmed_dataset['camera_extrinsics'] = dataset['camera_extrinsics']\n",
    "  trimmed_dataset['camera_intrinsics'] = dataset['camera_intrinsics']\n",
    "  trimmed_dataset['camera_distortions'] = dataset['camera_distortions']\n",
    "  trimmed_dataset['points_3d'] = dataset['points_3d']\n",
    "\n",
    "  for k in trimmed_dataset.keys():\n",
    "    if not isinstance(trimmed_dataset[k], torch.Tensor):\n",
    "      trimmed_dataset[k] = torch.from_numpy(trimmed_dataset[k])\n",
    "    trimmed_dataset[k] = trimmed_dataset[k].to(DEVICE)\n",
    "  return trimmed_dataset\n",
    "\n",
    "trimmed_dataset = trim_dataset(dataset, max_pixels=MAX_PIXELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojerr(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixels)\n",
    "\n",
    "  return loss\n",
    "\n",
    "def reprojerr_vmap(pose, point, intrinsic, distortion, pixel):\n",
    "  # reprojerr_vmap is not batched, it operates on a single 3D point and camera\n",
    "  pose = pp.LieTensor(pose, ltype=pp.SE3_type) # pose will lose its ltype through vmap, temporary fix\n",
    "  point = pose.unsqueeze(-2) @ point\n",
    "  point = point.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  point_proj = -point[:2] / point[-1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  f = intrinsic[0, 0]\n",
    "  k1 = distortion[0]\n",
    "  k2 = distortion[1]\n",
    "  n = torch.sum(point_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f * r * point_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixel)\n",
    "\n",
    "  return loss\n",
    "\n",
    "def reprojerr_gen(*args):\n",
    "    if args[4].shape[0] == MAX_PIXELS:\n",
    "        return reprojerr(*args)\n",
    "    else:\n",
    "        args = list(args)\n",
    "        return reprojerr_vmap(*args[:-2])\n",
    "\n",
    "# sparse version\n",
    "class ReprojNonBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr_gen(self.pose, self.points_3d, *args)\n",
    "\n",
    "def least_square_error(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the least square error\n",
    "  return (torch.flatten(img_repj - pixels) ** 2).sum() / 2\n",
    "\n",
    "def mean_square_error(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the mean suqared error\n",
    "  return (img_repj - pixels).norm(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loss: 5.668307781219482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/zzhan4/pypose/pypose/optim/functional.py:173: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)\n",
      "  dummy_csc = dummy_coo.coalesce().to_sparse_csc()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least square loss 5.724 @ 0 it\n",
      "Least square loss 5.793 @ 1 it\n",
      "Least square loss 5.998 @ 2 it\n",
      "Least square loss 6.241 @ 3 it\n",
      "Least square loss 6.554 @ 4 it\n",
      "Least square loss 7.047 @ 5 it\n",
      "Least square loss 7.584 @ 6 it\n",
      "Least square loss 8.249 @ 7 it\n",
      "Least square loss 9.059 @ 8 it\n",
      "Least square loss 10.079 @ 9 it\n",
      "Least square loss 11.010 @ 10 it\n",
      "Least square loss 12.474 @ 11 it\n",
      "Least square loss 18.170 @ 12 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 13 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 14 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 15 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 16 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 17 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 18 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 19 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 20 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 21 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 22 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 23 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 24 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 25 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 26 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 27 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 28 it\n",
      "CG failed to converge \n",
      "Linear solver failed. Breaking optimization step...\n",
      "Least square loss 18.170 @ 29 it\n",
      "Ending loss: 18.16964340209961\n"
     ]
    }
   ],
   "source": [
    "from pypose.optim.solver import CG\n",
    "input = [trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['points_2d'],\n",
    "         trimmed_dataset['camera_index_of_observations'],\n",
    "         trimmed_dataset['point_index_of_observations']]\n",
    "\n",
    "\n",
    "model_non_batched = ReprojNonBatched(trimmed_dataset['camera_extrinsics'].clone(),\n",
    "                                     trimmed_dataset['points_3d'].clone())\n",
    "model_non_batched.points_3d.requires_grad = False\n",
    "model_non_batched = model_non_batched.to(DEVICE)\n",
    "\n",
    "strategy_sparse = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, solver=CG(tol=1e-2), reject=1, dense_A=True, scipy=False)\n",
    "\n",
    "print('Starting loss:', mean_square_error(model_non_batched.pose, model_non_batched.points_3d, *input).item())\n",
    "for idx in range(30):\n",
    "    loss = optimizer_sparse.step(input)\n",
    "    print('Least square loss %.3f @ %d it'%(mean_square_error(model_non_batched.pose, model_non_batched.points_3d, *input).item(), idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n",
    "print('Ending loss:', mean_square_error(model_non_batched.pose, model_non_batched.points_3d, *input).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
