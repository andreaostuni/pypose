{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5RoSZ7kJg-W3"
   },
   "source": [
    "# Bundle Adjustment Example using PyPose and the BAL dataset\n",
    "\n",
    "```\n",
    "The dataset is from the following paper:  \n",
    "Sameer Agarwal, Noah Snavely, Steven M. Seitz, and Richard Szeliski.  \n",
    "Bundle adjustment in the large.  \n",
    "In European Conference on Computer Vision (ECCV), 2010.  \n",
    "```\n",
    "\n",
    "Link to the dataset: https://grail.cs.washington.edu/projects/bal/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse-Jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data for ladybug...\n",
      "Fetched problem-49-7776-pre from ladybug\n"
     ]
    }
   ],
   "source": [
    "from bal_loader import build_pipeline\n",
    "\n",
    "# TARGET_DATASET = \"trafalgar\"\n",
    "# TARGET_PROBLEM = \"problem-257-65132-pre\"\n",
    "# MAX_PIXELS = 225911\n",
    "\n",
    "TARGET_DATASET = \"ladybug\"\n",
    "TARGET_PROBLEM = \"problem-49-7776-pre\"\n",
    "MAX_PIXELS = 31843\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "def filter_problem(x):\n",
    "  return x['problem_name'] == TARGET_PROBLEM\n",
    "\n",
    "dataset_pipeline = build_pipeline(dataset=TARGET_DATASET, cache_dir='bal_data').filter(filter_problem)\n",
    "dataset_iterator = iter(dataset_pipeline)\n",
    "dataset = next(dataset_iterator)\n",
    "\n",
    "print(f'Fetched {TARGET_PROBLEM} from {TARGET_DATASET}')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pypose as pp\n",
    "\n",
    "def trim_dataset(dataset, max_pixels):\n",
    "  trimmed_dataset = dict()\n",
    "  trimmed_dataset['points_2d'] = dataset['points_2d'][:max_pixels]\n",
    "  trimmed_dataset['point_index_of_observations'] = dataset['point_index_of_observations'][:max_pixels]\n",
    "  trimmed_dataset['camera_index_of_observations'] = dataset['camera_index_of_observations'][:max_pixels]\n",
    "  # other fields are not changed\n",
    "  trimmed_dataset['camera_extrinsics'] = dataset['camera_extrinsics']\n",
    "  trimmed_dataset['camera_intrinsics'] = dataset['camera_intrinsics']\n",
    "  trimmed_dataset['camera_distortions'] = dataset['camera_distortions']\n",
    "  trimmed_dataset['points_3d'] = dataset['points_3d']\n",
    "\n",
    "  for k in trimmed_dataset.keys():\n",
    "    if not isinstance(trimmed_dataset[k], torch.Tensor):\n",
    "      trimmed_dataset[k] = torch.from_numpy(trimmed_dataset[k])\n",
    "    trimmed_dataset[k] = trimmed_dataset[k].to(DEVICE)\n",
    "  return trimmed_dataset\n",
    "\n",
    "def reprojerr(pose, points, pixels, intrinsics, distortions, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  # intrinsics = intrinsics[camera_index]\n",
    "  # distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixels).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "trimmed_dataset = trim_dataset(dataset, max_pixels=MAX_PIXELS)\n",
    "\n",
    "# errors = reprojerr(trimmed_dataset['camera_extrinsics'], # [49, 7], a LieTensor\n",
    "#                    trimmed_dataset['points_3d'], # [7776, 3]\n",
    "#                    trimmed_dataset['points_2d'], # [1000, 2] before trimmed [31843, 2]\n",
    "#                    trimmed_dataset['camera_intrinsics'], # [49, 3, 3]\n",
    "#                    trimmed_dataset['camera_distortions'],) # [49, 2\n",
    "                  #  trimmed_dataset['point_index_of_observations'], # [1000] this 2-D point is produced by which 3-D point\n",
    "                  #  trimmed_dataset['camera_index_of_observations']) # [1000] this 2-D point is produced by which camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojerr_vmap(pose, point, pixel, intrinsic, distortion):\n",
    "  # reprojerr_vmap is not batched, it operates on a single 3D point and camera\n",
    "  pose = pp.LieTensor(pose, ltype=pp.SE3_type) # pose will lose its ltype through vmap, temporary fix\n",
    "  point = pose.unsqueeze(-2) @ point\n",
    "  point = point.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  point_proj = -point[:2] / point[-1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  f = intrinsic[0, 0]\n",
    "  k1 = distortion[0]\n",
    "  k2 = distortion[1]\n",
    "  n = torch.sum(point_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f * r * point_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixel).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "# jac_function_vmap = torch.vmap(pp.func.jacrev(reprojerr_vmap))\n",
    "# jac_from_vmap = jac_function_vmap(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "#                       trimmed_dataset['points_2d'],\n",
    "#                       trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']])\n",
    "# print(jac_from_vmap.shape)\n",
    "\n",
    "def reprojerr_gen(*args):\n",
    "    if args[4].shape[0] == MAX_PIXELS:\n",
    "        return reprojerr(*args)\n",
    "    else:\n",
    "        args = list(args)\n",
    "        return reprojerr_vmap(*args[:-2])\n",
    "\n",
    "# sparse version\n",
    "class ReprojNonBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d):\n",
    "        super().__init__()\n",
    "        self.pose = pp.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr_gen(self.pose, self.points_3d, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densed version\n",
    "class ReprojBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr(self.pose, self.points_3d, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypose.optim.solver import Krylov\n",
    "input = [trimmed_dataset['points_2d'],\n",
    "         trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['camera_index_of_observations'],\n",
    "         trimmed_dataset['point_index_of_observations']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square_error(pose, points, pixels, intrinsics, distortions, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  # intrinsics = intrinsics[camera_index]\n",
    "  # distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the least square error\n",
    "  return (torch.flatten(img_repj - pixels) ** 2).sum() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loss: 850912.75\n",
      "Least square loss 1277107.000 @ 0 it\n",
      "Least square loss 1276469.250 @ 1 it\n",
      "Least square loss 1275283.250 @ 2 it\n",
      "Least square loss 1269541.500 @ 3 it\n",
      "Least square loss 1260798.625 @ 4 it\n",
      "Least square loss 1254494.375 @ 5 it\n",
      "Least square loss 1247155.125 @ 6 it\n",
      "Least square loss 1239018.500 @ 7 it\n",
      "Least square loss 1234315.750 @ 8 it\n",
      "Least square loss 1231778.250 @ 9 it\n",
      "Least square loss 1229343.875 @ 10 it\n",
      "Least square loss 1227006.000 @ 11 it\n",
      "Least square loss 1224723.000 @ 12 it\n",
      "Least square loss 1222579.500 @ 13 it\n",
      "Least square loss 1220205.375 @ 14 it\n",
      "Least square loss 1217971.500 @ 15 it\n",
      "Least square loss 1215594.500 @ 16 it\n",
      "Least square loss 1213365.125 @ 17 it\n",
      "Least square loss 1211166.000 @ 18 it\n",
      "Least square loss 1208939.250 @ 19 it\n",
      "Least square loss 1206652.250 @ 20 it\n",
      "Least square loss 1204486.000 @ 21 it\n",
      "Least square loss 1202202.875 @ 22 it\n",
      "Least square loss 1200186.750 @ 23 it\n",
      "Least square loss 1197985.250 @ 24 it\n",
      "Least square loss 1195808.250 @ 25 it\n",
      "Least square loss 1193807.000 @ 26 it\n",
      "Least square loss 1191859.250 @ 27 it\n",
      "Least square loss 1189624.125 @ 28 it\n",
      "Least square loss 1187514.375 @ 29 it\n",
      "Least square loss 1185288.625 @ 30 it\n",
      "Least square loss 1183237.500 @ 31 it\n",
      "Least square loss 1181054.125 @ 32 it\n",
      "Least square loss 1178989.625 @ 33 it\n",
      "Least square loss 1177004.000 @ 34 it\n",
      "Least square loss 1174972.750 @ 35 it\n",
      "Least square loss 1173007.750 @ 36 it\n",
      "Least square loss 1171100.375 @ 37 it\n",
      "Least square loss 1169273.500 @ 38 it\n",
      "Least square loss 1167212.750 @ 39 it\n",
      "Least square loss 1165097.750 @ 40 it\n",
      "Least square loss 1163113.750 @ 41 it\n",
      "Least square loss 1161027.250 @ 42 it\n",
      "Least square loss 1158967.000 @ 43 it\n",
      "Least square loss 1157019.000 @ 44 it\n",
      "Least square loss 1154972.500 @ 45 it\n",
      "Least square loss 1152959.000 @ 46 it\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_non_batched = ReprojNonBatched(trimmed_dataset['camera_extrinsics'].clone(),\n",
    "                                     trimmed_dataset['points_3d'].clone())\n",
    "\n",
    "model_non_batched = model_non_batched.to(DEVICE)\n",
    "\n",
    "strategy_sparse = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "#optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, solver=Krylov(rtol=3e-4, iterations=5000), reject=1)\n",
    "optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, dense_A=True)\n",
    "\n",
    "print('starting loss:', least_square_error(model_non_batched.pose, model_non_batched.points_3d, *input).item())\n",
    "for idx in range(100):\n",
    "    loss = optimizer_sparse.step(input)\n",
    "    # if (idx + 1) % 10 == 0:\n",
    "    print('Least square loss %.3f @ %d it'%(least_square_error(model_non_batched.pose, model_non_batched.points_3d, *input).item(), idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n",
    "print('ending loss:', least_square_error(model_non_batched.pose, model_non_batched.points_3d, *input).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(model_non_batched.named_parameters())\n",
    "points_3d_opt = params['points_3d'].to(device='cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.7799599e+00,  5.3873158e-01, -3.1275871e+00],\n",
       "       [ 1.8246309e+00,  5.4241651e-01, -3.1123295e+00],\n",
       "       [ 5.5786234e-01,  2.3620661e-01, -2.7201819e+00],\n",
       "       ...,\n",
       "       [ 1.3664684e+01,  2.2823713e+00,  2.1599278e+00],\n",
       "       [-8.1716841e-01,  4.9323239e-03, -2.6544741e-01],\n",
       "       [-5.9021598e-01,  2.1373612e-01, -1.3738620e+00]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Initial error: 2.45619e+07\n",
    "points_3d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('points_3d_opt_problem-257-65132-pre.npy', 'wb') as f:\n",
    "    np.save(f, points_3d_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_batched = ReprojBatched(trimmed_dataset['camera_extrinsics'].clone(), trimmed_dataset['points_3d'].clone())\n",
    "\n",
    "strategy_dense = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_dense = pp.optim.LM(model_batched, strategy=strategy_dense, sparse=False)\n",
    "for idx in range(3):\n",
    "    loss = optimizer_dense.step(input)\n",
    "    print('Pose Inversion loss %.7f @ %d it'%(loss, idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct SBT from vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([3, 3, 1, 3]), dense dim: 2, sparse: 2\n",
      "torch.Size([3, 9])\n",
      "torch.Size([9, 3])\n",
      "torch.Size([9, 9])\n",
      "torch.sparse.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "import pypose as pp\n",
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "v = torch.arange(12).view((-1, 1, 3)).to(dtype=torch.float32)\n",
    "x = pp.sbktensor(i, v, size=(3, 3), dtype=torch.float32)\n",
    "storage_t = x._s\n",
    "\n",
    "print(f'shape: {storage_t.shape}, dense dim: {storage_t.dense_dim()}, \\\n",
    "sparse: {storage_t.sparse_dim()}')\n",
    "flattened_coo = pp.hybrid2coo(storage_t) # the flattened tensor\n",
    "print(flattened_coo.shape)\n",
    "flattened_coo_T = flattened_coo.T\n",
    "print(flattened_coo_T.shape)\n",
    "A = torch.sparse.mm(flattened_coo_T, flattened_coo)\n",
    "print(A.shape)\n",
    "print(A.type())\n",
    "# pdt = torch.matmul(flattened_coo_T, torch.arange(1000).view(-1, 1).to(dtype=torch.float32))\n",
    "# print(pdt.shape)\n",
    "# print(pdt.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([ 0,  6, 12, 18, 24, 30, 36, 45, 54, 63]),\n",
       "       col_indices=tensor([0, 1, 2, 6, 7, 8, 0, 1, 2, 6, 7, 8, 0, 1, 2, 6, 7, 8,\n",
       "                           3, 4, 5, 6, 7, 8, 3, 4, 5, 6, 7, 8, 3, 4, 5, 6, 7, 8,\n",
       "                           0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                           0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       "       values=tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   2.,\n",
       "                        3.,   4.,   5.,   0.,   2.,   4.,   6.,   8.,  10.,\n",
       "                       36.,  42.,  48.,  54.,  60.,  66.,  42.,  49.,  56.,\n",
       "                       63.,  70.,  77.,  48.,  56.,  64.,  72.,  80.,  88.,\n",
       "                        0.,   3.,   6.,  54.,  63.,  72.,  90., 102., 114.,\n",
       "                        0.,   4.,   8.,  60.,  70.,  80., 102., 116., 130.,\n",
       "                        0.,   5.,  10.,  66.,  77.,  88., 114., 130., 146.]),\n",
       "       size=(9, 9), nnz=63, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.to_sparse_csr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "# v = torch.randn(4 * 2 * 2).view((4, 2, 2)).to(dtype=torch.float32)\n",
    "v = torch.randn(4)\n",
    "sct = torch.sparse_coo_tensor(i, v)\n",
    "v2 = torch.randn(4)\n",
    "sct2 = torch.sparse_coo_tensor(i, v2)\n",
    "# torch.cat([sct, sct2], dim=1)\n",
    "# pdt = torch.sparse.mm(v, v2)\n",
    "pdt = sct @ sct2.T\n",
    "print(pdt.type())\n",
    "print(pdt.is_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the jacobian values from jacrev and vmap match\n",
    "batch_dim = jac_from_jacrev.shape[0]\n",
    "index = trimmed_dataset['camera_index_of_observations']\n",
    "jac_from_jacrev_condensed = jac_from_jacrev[torch.arange(batch_dim), index]\n",
    "print(jac_from_jacrev_condensed.shape)\n",
    "if torch.allclose(jac_from_vmap, jac_from_jacrev_condensed):\n",
    "  print(\"Jacobian structure sanity check: ok!\")\n",
    "else:\n",
    "  print(\"Jacobian structure sanity check: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually construct SBT from vmap result\n",
    "import torch\n",
    "import pypose as pp\n",
    "from pypose.sparse import sbktensor\n",
    "\n",
    "def construct_sbt(jac_from_vmap, num_cameras, camera_index):\n",
    "  # camera_index = torch.from_numpy(camera_index) # for torch.stack\n",
    "  n = camera_index.shape[0] # num 2D points\n",
    "  # i = torch.stack([torch.arange(n), camera_index, torch.zeros(n)])\n",
    "  i = torch.stack([torch.arange(n), camera_index])\n",
    "  print(i.shape)\n",
    "  # v = jac_from_vmap[:, None, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  v = jac_from_vmap[:, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  # return pp.sbktensor(i, v, size=(n, num_cameras, 1), dtype=torch.float32)\n",
    "  return pp.sbktensor.sbktensor(i, v, size=(n, num_cameras), dtype=torch.float32)\n",
    "\n",
    "sparse_jac = construct_sbt(jac_from_vmap, len(trimmed_dataset['camera_extrinsics']), trimmed_dataset['camera_index_of_observations'])\n",
    "dense_jac = sparse_jac.to_dense()\n",
    "# if torch.allclose(jac_from_jacrev, dense_jac):\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: ok!\")\n",
    "# else:\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparse_jac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m storage_t \u001b[39m=\u001b[39m sparse_jac\u001b[39m.\u001b[39m_s\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mshape: \u001b[39m\u001b[39m{\u001b[39;00mstorage_t\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, dense dim: \u001b[39m\u001b[39m{\u001b[39;00mstorage_t\u001b[39m.\u001b[39mdense_dim()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39msparse: \u001b[39m\u001b[39m{\u001b[39;00mstorage_t\u001b[39m.\u001b[39msparse_dim()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m flattened_coo \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39mhybrid2coo(storage_t) \u001b[39m# the flattened tensor\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparse_jac' is not defined"
     ]
    }
   ],
   "source": [
    "storage_t = sparse_jac._s\n",
    "print(f'shape: {storage_t.shape}, dense dim: {storage_t.dense_dim()}, \\\n",
    "sparse: {storage_t.sparse_dim()}')\n",
    "flattened_coo = pp.hybrid2coo(storage_t) # the flattened tensor\n",
    "print(flattened_coo.shape)\n",
    "flattened_coo_T = flattened_coo.T\n",
    "print(flattened_coo_T.shape)\n",
    "A = torch.sparse.mm(flattened_coo_T, flattened_coo)\n",
    "print(A.shape)\n",
    "print(A.type())\n",
    "# pdt = torch.matmul(flattened_coo_T, torch.arange(1000).view(-1, 1).to(dtype=torch.float32))\n",
    "# print(pdt.shape)\n",
    "# print(pdt.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_coo_diagonal(t: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    return t.values()[diag_indices]\n",
    "\n",
    "def sparse_coo_diagonal_clamp_(t: torch.Tensor, min_value, max_value):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].clamp_(min_value, max_value)\n",
    "\n",
    "\n",
    "def sparse_coo_diagonal_add_(t: torch.Tensor, other: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].add_(other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_indices = (A.indices()[0] == A.indices()[1]).nonzero(as_tuple=True)\n",
    "# print(diag_indices[0][])\n",
    "sparse_coo_diagonal_clamp_(A, -100, 100)\n",
    "print(A.values()[diag_indices[0][:10]])\n",
    "sparse_coo_diagonal_add_(A, sparse_coo_diagonal(A) * torch.ones(301) * 2)\n",
    "print(A.values()[diag_indices[0][:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacrev_custom(func, argnums):\n",
    "  def wrapper(*args, **kwargs):\n",
    "    jac_vmap = torch.vmap(pp.func.jacrev(func)) # vmap\n",
    "    gradients = jac_vmap(*args)\n",
    "    sbt = construct_sbt(gradients, kwargs['num_cols'], kwargs['index'])\n",
    "    return sbt\n",
    "  return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_function_custom = jacrev_custom(reprojerr_vmap, argnums=0)\n",
    "jac_from_custom = jac_function_custom(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "                      trimmed_dataset['points_2d'],\n",
    "                      trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      num_cols=len(trimmed_dataset['camera_extrinsics']),\n",
    "                      index=trimmed_dataset['camera_index_of_observations'])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pypose",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
