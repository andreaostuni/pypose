{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5RoSZ7kJg-W3"
   },
   "source": [
    "# Bundle Adjustment Example using PyPose and the BAL dataset\n",
    "\n",
    "```\n",
    "The dataset is from the following paper:  \n",
    "Sameer Agarwal, Noah Snavely, Steven M. Seitz, and Richard Szeliski.  \n",
    "Bundle adjustment in the large.  \n",
    "In European Conference on Computer Vision (ECCV), 2010.  \n",
    "```\n",
    "\n",
    "Link to the dataset: https://grail.cs.washington.edu/projects/bal/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse-Jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data for trafalgar...\n",
      "Fetched problem-126-40037-pre from trafalgar\n"
     ]
    }
   ],
   "source": [
    "from bal_loader import build_pipeline\n",
    "\n",
    "TARGET_DATASET = \"trafalgar\"\n",
    "TARGET_PROBLEM = \"problem-126-40037-pre\"\n",
    "MAX_PIXELS = 148328\n",
    "\n",
    "# TARGET_DATASET = \"ladybug\"\n",
    "# TARGET_PROBLEM = \"problem-49-7776-pre\"\n",
    "# MAX_PIXELS = 31843\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "def filter_problem(x):\n",
    "  return x['problem_name'] == TARGET_PROBLEM\n",
    "\n",
    "dataset_pipeline = build_pipeline(dataset=TARGET_DATASET, cache_dir='bal_data').filter(filter_problem)\n",
    "dataset_iterator = iter(dataset_pipeline)\n",
    "dataset = next(dataset_iterator)\n",
    "\n",
    "print(f'Fetched {TARGET_PROBLEM} from {TARGET_DATASET}')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pypose as pp\n",
    "\n",
    "def trim_dataset(dataset, max_pixels):\n",
    "  trimmed_dataset = dict()\n",
    "  trimmed_dataset['points_2d'] = dataset['points_2d'][:max_pixels]\n",
    "  trimmed_dataset['point_index_of_observations'] = dataset['point_index_of_observations'][:max_pixels]\n",
    "  trimmed_dataset['camera_index_of_observations'] = dataset['camera_index_of_observations'][:max_pixels]\n",
    "  # other fields are not changed\n",
    "  trimmed_dataset['camera_extrinsics'] = dataset['camera_extrinsics']\n",
    "  trimmed_dataset['camera_intrinsics'] = dataset['camera_intrinsics']\n",
    "  trimmed_dataset['camera_distortions'] = dataset['camera_distortions']\n",
    "  trimmed_dataset['points_3d'] = dataset['points_3d']\n",
    "\n",
    "  for k in trimmed_dataset.keys():\n",
    "    if not isinstance(trimmed_dataset[k], torch.Tensor):\n",
    "      trimmed_dataset[k] = torch.from_numpy(trimmed_dataset[k])\n",
    "    trimmed_dataset[k] = trimmed_dataset[k].to(DEVICE)\n",
    "  return trimmed_dataset\n",
    "\n",
    "def reprojerr(pose, points, pixels, intrinsics, distortions, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  # intrinsics = intrinsics[camera_index]\n",
    "  # distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixels).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "trimmed_dataset = trim_dataset(dataset, max_pixels=MAX_PIXELS)\n",
    "\n",
    "# errors = reprojerr(trimmed_dataset['camera_extrinsics'], # [49, 7], a LieTensor\n",
    "#                    trimmed_dataset['points_3d'], # [7776, 3]\n",
    "#                    trimmed_dataset['points_2d'], # [1000, 2] before trimmed [31843, 2]\n",
    "#                    trimmed_dataset['camera_intrinsics'], # [49, 3, 3]\n",
    "#                    trimmed_dataset['camera_distortions'],) # [49, 2\n",
    "                  #  trimmed_dataset['point_index_of_observations'], # [1000] this 2-D point is produced by which 3-D point\n",
    "                  #  trimmed_dataset['camera_index_of_observations']) # [1000] this 2-D point is produced by which camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojerr_vmap(pose, point, pixel, intrinsic, distortion):\n",
    "  # reprojerr_vmap is not batched, it operates on a single 3D point and camera\n",
    "  pose = pp.LieTensor(pose, ltype=pp.SE3_type) # pose will lose its ltype through vmap, temporary fix\n",
    "  point = pose.unsqueeze(-2) @ point\n",
    "  point = point.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  point_proj = -point[:2] / point[-1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  f = intrinsic[0, 0]\n",
    "  k1 = distortion[0]\n",
    "  k2 = distortion[1]\n",
    "  n = torch.sum(point_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f * r * point_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixel).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "# jac_function_vmap = torch.vmap(pp.func.jacrev(reprojerr_vmap))\n",
    "# jac_from_vmap = jac_function_vmap(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "#                       trimmed_dataset['points_2d'],\n",
    "#                       trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']])\n",
    "# print(jac_from_vmap.shape)\n",
    "\n",
    "def reprojerr_gen(*args):\n",
    "    if args[2].shape[0] == MAX_PIXELS:\n",
    "        return reprojerr(*args)\n",
    "    else:\n",
    "        args = list(args)\n",
    "        return reprojerr_vmap(*args[:-2])\n",
    "\n",
    "# sparse version\n",
    "class ReprojNonBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d):\n",
    "        super().__init__()\n",
    "        self.pose = pp.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr_gen(self.pose, self.points_3d, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densed version\n",
    "class ReprojBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr(self.pose, self.points_3d, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypose.optim.solver import Krylov\n",
    "input = [trimmed_dataset['points_2d'],\n",
    "         trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['camera_index_of_observations'],\n",
    "         trimmed_dataset['point_index_of_observations']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loss: 1206351.25\n",
      "matvec percentage: 93.44%\n",
      "accept step after 0 retries\n",
      "step time percentage: 99.03%\n",
      "Pose Inversion loss 19889620.0000000 @ 0 it\n",
      "matvec percentage: 65.98%\n",
      "accept step after 0 retries\n",
      "step time percentage: 14.36%\n",
      "Pose Inversion loss 19328582.0000000 @ 1 it\n",
      "matvec percentage: 54.25%\n",
      "reject step 1\n",
      "matvec percentage: 53.73%\n",
      "reject step 2\n",
      "matvec percentage: 52.59%\n",
      "reject step 3\n",
      "matvec percentage: 52.63%\n",
      "accept step after 3 retries\n",
      "step time percentage: 26.78%\n",
      "Pose Inversion loss 19328586.0000000 @ 2 it\n",
      "matvec percentage: 50.83%\n",
      "reject step 1\n",
      "matvec percentage: 58.31%\n",
      "reject step 2\n",
      "matvec percentage: 52.05%\n",
      "reject step 3\n",
      "matvec percentage: 46.66%\n",
      "accept step after 3 retries\n",
      "step time percentage: 27.82%\n",
      "Pose Inversion loss 19328598.0000000 @ 3 it\n",
      "matvec percentage: 51.07%\n",
      "reject step 1\n",
      "matvec percentage: 53.70%\n",
      "reject step 2\n",
      "matvec percentage: 54.37%\n",
      "reject step 3\n",
      "matvec percentage: 52.44%\n",
      "accept step after 3 retries\n",
      "step time percentage: 26.72%\n",
      "Pose Inversion loss 19328622.0000000 @ 4 it\n",
      "matvec percentage: 53.46%\n",
      "reject step 1\n",
      "matvec percentage: 50.15%\n",
      "reject step 2\n",
      "matvec percentage: 53.55%\n",
      "reject step 3\n",
      "matvec percentage: 54.06%\n",
      "accept step after 3 retries\n",
      "step time percentage: 27.49%\n",
      "Pose Inversion loss 19328662.0000000 @ 5 it\n",
      "matvec percentage: 53.20%\n",
      "reject step 1\n",
      "matvec percentage: 53.88%\n",
      "accept step after 1 retries\n",
      "step time percentage: 19.84%\n",
      "Pose Inversion loss 19328646.0000000 @ 6 it\n",
      "matvec percentage: 52.17%\n",
      "reject step 1\n",
      "matvec percentage: 54.73%\n",
      "accept step after 1 retries\n",
      "step time percentage: 20.05%\n",
      "Pose Inversion loss 19328616.0000000 @ 7 it\n",
      "matvec percentage: 52.66%\n",
      "accept step after 0 retries\n",
      "step time percentage: 12.69%\n",
      "Pose Inversion loss 19328608.0000000 @ 8 it\n",
      "matvec percentage: 52.78%\n",
      "accept step after 0 retries\n",
      "step time percentage: 12.81%\n",
      "Pose Inversion loss 19328608.0000000 @ 9 it\n",
      "matvec percentage: 47.76%\n",
      "accept step after 0 retries\n",
      "step time percentage: 13.50%\n",
      "Pose Inversion loss 19328588.0000000 @ 10 it\n",
      "matvec percentage: 52.19%\n",
      "accept step after 0 retries\n",
      "step time percentage: 12.89%\n",
      "Pose Inversion loss 19328582.0000000 @ 11 it\n",
      "matvec percentage: 52.83%\n",
      "accept step after 0 retries\n",
      "step time percentage: 12.99%\n",
      "Pose Inversion loss 19328578.0000000 @ 12 it\n",
      "matvec percentage: 53.68%\n",
      "accept step after 0 retries\n",
      "step time percentage: 12.89%\n",
      "Pose Inversion loss 19328578.0000000 @ 13 it\n",
      "matvec percentage: 85.02%\n",
      "accept step after 0 retries\n",
      "step time percentage: 31.19%\n",
      "Pose Inversion loss 17158180.0000000 @ 14 it\n",
      "matvec percentage: 92.75%\n",
      "accept step after 0 retries\n",
      "step time percentage: 76.22%\n",
      "Pose Inversion loss 12797185.0000000 @ 15 it\n",
      "matvec percentage: 93.03%\n",
      "accept step after 0 retries\n",
      "step time percentage: 89.97%\n",
      "Pose Inversion loss 12462808.0000000 @ 16 it\n",
      "matvec percentage: 91.60%\n",
      "accept step after 0 retries\n",
      "step time percentage: 77.60%\n",
      "Pose Inversion loss 11732140.0000000 @ 17 it\n",
      "matvec percentage: 91.20%\n",
      "accept step after 0 retries\n",
      "step time percentage: 73.20%\n",
      "Pose Inversion loss 11299878.0000000 @ 18 it\n",
      "matvec percentage: 89.89%\n",
      "reject step 1\n",
      "matvec percentage: 93.13%\n",
      "accept step after 1 retries\n",
      "step time percentage: 85.56%\n",
      "Pose Inversion loss 10801026.0000000 @ 19 it\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstarting loss:\u001b[39m\u001b[39m'\u001b[39m, model_non_batched\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m35\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss \u001b[39m=\u001b[39m optimizer_sparse\u001b[39m.\u001b[39;49mstep(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# if (idx + 1) % 10 == 0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPose Inversion loss \u001b[39m\u001b[39m%.7f\u001b[39;00m\u001b[39m @ \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m it\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(loss, idx))\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/current/github/pypose.nosync/pypose/optim/optimizer.py:534\u001b[0m, in \u001b[0;36mLevenbergMarquardt.step\u001b[0;34m(self, input, target, weight)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    533\u001b[0m     solver_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 534\u001b[0m     D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver(A \u001b[39m=\u001b[39;49m A, b \u001b[39m=\u001b[39;49m \u001b[39m-\u001b[39;49mJ_T \u001b[39m@\u001b[39;49m R\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m    535\u001b[0m     D \u001b[39m=\u001b[39m D\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    536\u001b[0m     \u001b[39m#D = D.to(torch.float32)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/current/github/pypose.nosync/pypose/optim/solver.py:290\u001b[0m, in \u001b[0;36mKrylov.forward\u001b[0;34m(self, A, b, x, M)\u001b[0m\n\u001b[1;32m    288\u001b[0m r \u001b[39m=\u001b[39m r \u001b[39m-\u001b[39m alpha \u001b[39m*\u001b[39m Ap \u001b[39m# r.shape = [120993, 1]\u001b[39;00m\n\u001b[1;32m    289\u001b[0m matvec_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 290\u001b[0m z \u001b[39m=\u001b[39m M\u001b[39m@r\u001b[39;49m \u001b[39m# z.shape = [120993, 1]\u001b[39;00m\n\u001b[1;32m    291\u001b[0m matvec_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatvec_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m matvec_end \u001b[39m-\u001b[39m matvec_start\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "model_non_batched = ReprojNonBatched(trimmed_dataset['camera_extrinsics'].clone(),\n",
    "                                     trimmed_dataset['points_3d'].clone())\n",
    "\n",
    "model_non_batched = model_non_batched.to(DEVICE)\n",
    "\n",
    "strategy_sparse = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, solver=Krylov(rtol=1e-2, iterations=5000, debug=True), reject=3, debug=True)\n",
    "\n",
    "print('starting loss:', model_non_batched.forward(*input).sum().item())\n",
    "for idx in range(35):\n",
    "    loss = optimizer_sparse.step(input)\n",
    "    # if (idx + 1) % 10 == 0:\n",
    "    print('Pose Inversion loss %.7f @ %d it'%(loss, idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n",
    "print('ending loss:', model_non_batched.forward(*input).sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "model_batched = ReprojBatched(trimmed_dataset['camera_extrinsics'].clone(), trimmed_dataset['points_3d'].clone())\n",
    "\n",
    "strategy_dense = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_dense = pp.optim.LM(model_batched, strategy=strategy_dense, sparse=False)\n",
    "for idx in range(3):\n",
    "    loss = optimizer_dense.step(input)\n",
    "    print('Pose Inversion loss %.7f @ %d it'%(loss, idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct SBT from vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([3, 3, 1, 3]), dense dim: 2, sparse: 2\n",
      "torch.Size([3, 9])\n",
      "torch.Size([9, 3])\n",
      "torch.Size([9, 9])\n",
      "torch.sparse.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "import pypose as pp\n",
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "v = torch.arange(12).view((-1, 1, 3)).to(dtype=torch.float32)\n",
    "x = pp.sbktensor(i, v, size=(3, 3), dtype=torch.float32)\n",
    "storage_t = x._s\n",
    "\n",
    "print(f'shape: {storage_t.shape}, dense dim: {storage_t.dense_dim()}, \\\n",
    "sparse: {storage_t.sparse_dim()}')\n",
    "flattened_coo = pp.hybrid2coo(storage_t) # the flattened tensor\n",
    "print(flattened_coo.shape)\n",
    "flattened_coo_T = flattened_coo.T\n",
    "print(flattened_coo_T.shape)\n",
    "A = torch.sparse.mm(flattened_coo_T, flattened_coo)\n",
    "print(A.shape)\n",
    "print(A.type())\n",
    "# pdt = torch.matmul(flattened_coo_T, torch.arange(1000).view(-1, 1).to(dtype=torch.float32))\n",
    "# print(pdt.shape)\n",
    "# print(pdt.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([ 0,  6, 12, 18, 24, 30, 36, 45, 54, 63]),\n",
       "       col_indices=tensor([0, 1, 2, 6, 7, 8, 0, 1, 2, 6, 7, 8, 0, 1, 2, 6, 7, 8,\n",
       "                           3, 4, 5, 6, 7, 8, 3, 4, 5, 6, 7, 8, 3, 4, 5, 6, 7, 8,\n",
       "                           0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                           0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       "       values=tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   2.,\n",
       "                        3.,   4.,   5.,   0.,   2.,   4.,   6.,   8.,  10.,\n",
       "                       36.,  42.,  48.,  54.,  60.,  66.,  42.,  49.,  56.,\n",
       "                       63.,  70.,  77.,  48.,  56.,  64.,  72.,  80.,  88.,\n",
       "                        0.,   3.,   6.,  54.,  63.,  72.,  90., 102., 114.,\n",
       "                        0.,   4.,   8.,  60.,  70.,  80., 102., 116., 130.,\n",
       "                        0.,   5.,  10.,  66.,  77.,  88., 114., 130., 146.]),\n",
       "       size=(9, 9), nnz=63, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.to_sparse_csr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "# v = torch.randn(4 * 2 * 2).view((4, 2, 2)).to(dtype=torch.float32)\n",
    "v = torch.randn(4)\n",
    "sct = torch.sparse_coo_tensor(i, v)\n",
    "v2 = torch.randn(4)\n",
    "sct2 = torch.sparse_coo_tensor(i, v2)\n",
    "# torch.cat([sct, sct2], dim=1)\n",
    "# pdt = torch.sparse.mm(v, v2)\n",
    "pdt = sct @ sct2.T\n",
    "print(pdt.type())\n",
    "print(pdt.is_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the jacobian values from jacrev and vmap match\n",
    "batch_dim = jac_from_jacrev.shape[0]\n",
    "index = trimmed_dataset['camera_index_of_observations']\n",
    "jac_from_jacrev_condensed = jac_from_jacrev[torch.arange(batch_dim), index]\n",
    "print(jac_from_jacrev_condensed.shape)\n",
    "if torch.allclose(jac_from_vmap, jac_from_jacrev_condensed):\n",
    "  print(\"Jacobian structure sanity check: ok!\")\n",
    "else:\n",
    "  print(\"Jacobian structure sanity check: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually construct SBT from vmap result\n",
    "import torch\n",
    "import pypose as pp\n",
    "from pypose.sparse import sbktensor\n",
    "\n",
    "def construct_sbt(jac_from_vmap, num_cameras, camera_index):\n",
    "  # camera_index = torch.from_numpy(camera_index) # for torch.stack\n",
    "  n = camera_index.shape[0] # num 2D points\n",
    "  # i = torch.stack([torch.arange(n), camera_index, torch.zeros(n)])\n",
    "  i = torch.stack([torch.arange(n), camera_index])\n",
    "  print(i.shape)\n",
    "  # v = jac_from_vmap[:, None, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  v = jac_from_vmap[:, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  # return pp.sbktensor(i, v, size=(n, num_cameras, 1), dtype=torch.float32)\n",
    "  return pp.sbktensor.sbktensor(i, v, size=(n, num_cameras), dtype=torch.float32)\n",
    "\n",
    "sparse_jac = construct_sbt(jac_from_vmap, len(trimmed_dataset['camera_extrinsics']), trimmed_dataset['camera_index_of_observations'])\n",
    "dense_jac = sparse_jac.to_dense()\n",
    "# if torch.allclose(jac_from_jacrev, dense_jac):\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: ok!\")\n",
    "# else:\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparse_jac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m storage_t \u001b[39m=\u001b[39m sparse_jac\u001b[39m.\u001b[39m_s\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mshape: \u001b[39m\u001b[39m{\u001b[39;00mstorage_t\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, dense dim: \u001b[39m\u001b[39m{\u001b[39;00mstorage_t\u001b[39m.\u001b[39mdense_dim()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39msparse: \u001b[39m\u001b[39m{\u001b[39;00mstorage_t\u001b[39m.\u001b[39msparse_dim()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huanxu/Desktop/current/github/pypose.nosync/examples/sparse/ba_example.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m flattened_coo \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39mhybrid2coo(storage_t) \u001b[39m# the flattened tensor\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparse_jac' is not defined"
     ]
    }
   ],
   "source": [
    "storage_t = sparse_jac._s\n",
    "print(f'shape: {storage_t.shape}, dense dim: {storage_t.dense_dim()}, \\\n",
    "sparse: {storage_t.sparse_dim()}')\n",
    "flattened_coo = pp.hybrid2coo(storage_t) # the flattened tensor\n",
    "print(flattened_coo.shape)\n",
    "flattened_coo_T = flattened_coo.T\n",
    "print(flattened_coo_T.shape)\n",
    "A = torch.sparse.mm(flattened_coo_T, flattened_coo)\n",
    "print(A.shape)\n",
    "print(A.type())\n",
    "# pdt = torch.matmul(flattened_coo_T, torch.arange(1000).view(-1, 1).to(dtype=torch.float32))\n",
    "# print(pdt.shape)\n",
    "# print(pdt.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_coo_diagonal(t: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    return t.values()[diag_indices]\n",
    "\n",
    "def sparse_coo_diagonal_clamp_(t: torch.Tensor, min_value, max_value):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].clamp_(min_value, max_value)\n",
    "\n",
    "\n",
    "def sparse_coo_diagonal_add_(t: torch.Tensor, other: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].add_(other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_indices = (A.indices()[0] == A.indices()[1]).nonzero(as_tuple=True)\n",
    "# print(diag_indices[0][])\n",
    "sparse_coo_diagonal_clamp_(A, -100, 100)\n",
    "print(A.values()[diag_indices[0][:10]])\n",
    "sparse_coo_diagonal_add_(A, sparse_coo_diagonal(A) * torch.ones(301) * 2)\n",
    "print(A.values()[diag_indices[0][:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacrev_custom(func, argnums):\n",
    "  def wrapper(*args, **kwargs):\n",
    "    jac_vmap = torch.vmap(pp.func.jacrev(func)) # vmap\n",
    "    gradients = jac_vmap(*args)\n",
    "    sbt = construct_sbt(gradients, kwargs['num_cols'], kwargs['index'])\n",
    "    return sbt\n",
    "  return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_function_custom = jacrev_custom(reprojerr_vmap, argnums=0)\n",
    "jac_from_custom = jac_function_custom(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "                      trimmed_dataset['points_2d'],\n",
    "                      trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      num_cols=len(trimmed_dataset['camera_extrinsics']),\n",
    "                      index=trimmed_dataset['camera_index_of_observations'])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
