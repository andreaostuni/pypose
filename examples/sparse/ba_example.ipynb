{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import warnings\n",
    "import torch\n",
    "from torch.library import Library, impl\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def _bsr_diag(input, offset: int=0):\n",
    "    crow_indices = input.crow_indices() # b + 1 dimensional\n",
    "    col_indices = input.col_indices() # b + 1 dimensional\n",
    "    bsr_values = input.values() # 1 + 2 dimensional\n",
    "    m, n = input.shape[-2], input.shape[-1]\n",
    "    dense_m, dense_n = (bsr_values.shape[-2],\n",
    "                                 bsr_values.shape[-1])\n",
    "    sparse_m, sparse_n = m // dense_m, n // dense_n\n",
    "\n",
    "    #simple case(block is square and offset is 0)\n",
    "    if dense_m == dense_n and offset == 0:\n",
    "        dummy_val = torch.zeros(bsr_values.shape[0])\n",
    "        dummy = torch.sparse_csr_tensor(crow_indices=crow_indices,\n",
    "                                        col_indices=col_indices,\n",
    "                                        values=dummy_val)\n",
    "        dummy_coo = dummy.to_sparse(layout=torch.sparse_coo).coalesce()\n",
    "\n",
    "        indices = dummy_coo.indices()\n",
    "        diag_indices = indices[0] == indices[1]\n",
    "        values = bsr_values[diag_indices]\n",
    "        n_diag_blocks = sparse_m if sparse_m < sparse_n else sparse_n\n",
    "        results_shape = (n_diag_blocks, dense_m)\n",
    "        results = torch.zeros(results_shape, dtype=values.dtype, device=values.device)\n",
    "        results[indices[0, diag_indices]] = torch.diagonal(values, dim1=-2, dim2=-1)\n",
    "        results = torch.flatten(results)\n",
    "        return results\n",
    "\n",
    "def _sparse_csr_mm(mat1, mat2):\n",
    "    if isinstance(mat1, torch.Tensor) and mat1.layout == torch.sparse_bsr:\n",
    "        if isinstance(mat2, torch.Tensor) and mat2.layout == torch.sparse_bsc:\n",
    "            return bsr_bsc_matmul(mat1, mat2)\n",
    "    elif isinstance(mat1, torch.Tensor) and mat1.layout == torch.sparse_bsc:\n",
    "        if isinstance(mat2, torch.Tensor) and mat2.layout == torch.sparse_bsr:\n",
    "            raise NotImplemented\n",
    "    #https://github.com/pytorch/pytorch/blob/3fa3ed4923c19a2b8d2da69e994169b4c8ac5fe3/\n",
    "    #aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp#L789\n",
    "    if mat1.is_sparse_csr and mat2.is_sparse_csr:\n",
    "        return torch.addmm(\n",
    "            torch.zeros([mat1.size(0), mat2.size(1)], dtype=mat2.dtype,\n",
    "                device=mat2.device, layout=mat2.layout),\n",
    "            mat1,\n",
    "            mat2,\n",
    "            beta=0.0,\n",
    "            alpha=1.0)\n",
    "    if (mat1.layout == torch.sparse_csc or mat1.layout == torch.sparse_csr) and\\\n",
    "        (mat2.layout == torch.sparse_csc or mat2.layout == torch.sparse_csr):\n",
    "        return _sparse_csr_mm(mat1.to_sparse_csr(), mat2.to_sparse_csr())\n",
    "    if mat1.layout == torch.sparse_csc and mat2.layout == torch.strided:\n",
    "        return _sparse_csr_mm(mat1.to_sparse_csr(), mat2)\n",
    "    if mat2.layout == torch.strided:\n",
    "        return torch.addmm(\n",
    "            torch.zeros([mat1.size(0), mat2.size(1)], dtype=mat1.dtype,\n",
    "                device=mat1.device, layout=mat2.layout),\n",
    "            mat1,\n",
    "            mat2,\n",
    "            beta=0.0,\n",
    "            alpha=1.0)\n",
    "    return torch.addmm(\n",
    "        torch.zeros([mat1.size(0), mat2.size(1)], dtype=mat1.dtype, device=mat1.device,\n",
    "        layout=mat1.layout),\n",
    "        mat1,\n",
    "        mat2,\n",
    "        beta=0.0,\n",
    "        alpha=1.0)\n",
    "\n",
    "@torch.jit.script\n",
    "def bsr_bsc_matmul(bsr:torch.Tensor, bsc:torch.Tensor):\n",
    "    assert bsr.shape[-1] == bsc.shape[-2]\n",
    "    assert bsr.layout == torch.sparse_bsr or bsr.layout == torch.sparse_csr\n",
    "    assert bsc.layout == torch.sparse_bsc or bsc.layout == torch.sparse_csc\n",
    "    crow_indices = bsr.crow_indices() # b + 1 dimensional\n",
    "    col_indices = bsr.col_indices() # b + 1 dimensional\n",
    "    csr_values = bsr.values() # 1 + 2 dimensional\n",
    "\n",
    "    ccol_indices = bsc.ccol_indices() # b + 1 dimensional\n",
    "    row_indices = bsc.row_indices() # b + 1 dimensional\n",
    "    csc_values = bsc.values() # 1 + 2 dimensional\n",
    "\n",
    "    idx_dtype = crow_indices.dtype\n",
    "\n",
    "    assert bsr.ndim == 2 and bsc.ndim == 2, \"bsr and bsc must be 2 dimensional. \\\n",
    "    batch dimension is yet not supported.\"\n",
    "    m, n, p = bsr.shape[-2], bsr.shape[-1], bsc.shape[-1]\n",
    "    dense_m, dense_n, dense_p = (csr_values.shape[-2],\n",
    "                                 csr_values.shape[-1],\n",
    "                                 csc_values.shape[-1])\n",
    "    sparse_m, sparse_n, sparse_p = m // dense_m, n // dense_n, p // dense_p\n",
    "    assert dense_m * sparse_m == m\n",
    "    assert dense_n * sparse_n == n\n",
    "    assert dense_p * sparse_p == p\n",
    "\n",
    "    result_step: int = 0\n",
    "    coo_indices: List[int] = list()\n",
    "    index: List[int] = list()\n",
    "    source: List[int] = list()\n",
    "    for i in range(sparse_m):\n",
    "        for j in range(sparse_p):\n",
    "            nz: bool = False\n",
    "            k2 = int(ccol_indices[j].item())\n",
    "            for k1 in range(int(crow_indices[i].item()), int(crow_indices[i+1].item())):\n",
    "                if k2 == ccol_indices[j+1]:\n",
    "                    break\n",
    "                while row_indices[k2] < col_indices[k1] and k2 < ccol_indices[j+1] - 1:\n",
    "                    k2 += 1\n",
    "                if row_indices[k2] == col_indices[k1]:\n",
    "                    index.append(result_step)\n",
    "                    source.append(k1)\n",
    "                    source.append(k2)\n",
    "                    nz = True\n",
    "            if nz:\n",
    "                result_step += 1\n",
    "                coo_indices.append(i)\n",
    "                coo_indices.append(j)\n",
    "    source = torch.tensor(source, dtype=idx_dtype, device=bsr.device).view(-1, 2)\n",
    "    index = torch.tensor(index, dtype=idx_dtype, device=bsr.device)\n",
    "    prod = torch.bmm(csr_values[source[:, 0]], csc_values[source[:, 1]])\n",
    "    values_shape = (result_step, dense_m, dense_p)\n",
    "    reduced = torch.zeros(values_shape, dtype=prod.dtype, device=prod.device)\n",
    "    reduced.scatter_add_(0, index.unsqueeze(-1).unsqueeze(-1).expand_as(prod), prod)\n",
    "    coo_indices = torch.tensor(coo_indices, dtype=idx_dtype, device=bsr.device)\n",
    "    coo_indices = coo_indices.view(-1, 2).T\n",
    "    # use fake coo\n",
    "    dummy_val = torch.zeros(coo_indices.shape[-1], dtype=prod.dtype, device=prod.device)\n",
    "    dummy = torch.sparse_coo_tensor(indices=coo_indices,\n",
    "                                    values=dummy_val,\n",
    "                                    size=(sparse_m, sparse_p)).coalesce()\n",
    "    dummy_csr = dummy.to_sparse_csr()\n",
    "    return torch.sparse_bsr_tensor(dummy_csr.crow_indices(),\n",
    "                                   dummy_csr.col_indices(),\n",
    "                                   reduced,\n",
    "                                   size=(m, p), dtype=reduced.dtype)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    sparse_lib = Library('aten', 'IMPL')\n",
    "    sparse_lib.impl('mm', _sparse_csr_mm, 'SparseCsrCPU')\n",
    "    sparse_lib.impl('mm', _sparse_csr_mm, 'SparseCsrCUDA')\n",
    "    sparse_lib.impl('diagonal', _bsr_diag, 'SparseCsrCPU')\n",
    "    sparse_lib.impl('diagonal', _bsr_diag, 'SparseCsrCUDA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5RoSZ7kJg-W3"
   },
   "source": [
    "# Bundle Adjustment Example using SparsePyBA and the BAL dataset\n",
    "\n",
    "```\n",
    "The dataset is from the following paper:  \n",
    "Sameer Agarwal, Noah Snavely, Steven M. Seitz, and Richard Szeliski.  \n",
    "Bundle adjustment in the large.  \n",
    "In European Conference on Computer Vision (ECCV), 2010.  \n",
    "```\n",
    "\n",
    "Link to the dataset: https://grail.cs.washington.edu/projects/bal/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data for ladybug...\n",
      "Fetched problem-49-7776-pre from ladybug\n"
     ]
    }
   ],
   "source": [
    "from bal_loader import build_pipeline\n",
    "\n",
    "TARGET_DATASET = \"ladybug\"\n",
    "TARGET_PROBLEM = \"problem-49-7776-pre\"\n",
    "MAX_PIXELS = 31843\n",
    "\n",
    "# TARGET_DATASET = \"trafalgar\"\n",
    "# TARGET_PROBLEM = \"problem-257-65132-pre\"\n",
    "# MAX_PIXELS = 225911\n",
    "\n",
    "DEVICE = 'cpu' # change device to CPU if needed\n",
    "\n",
    "def filter_problem(x):\n",
    "  return x['problem_name'] == TARGET_PROBLEM\n",
    "\n",
    "dataset_pipeline = build_pipeline(dataset=TARGET_DATASET, cache_dir='bal_data').filter(filter_problem)\n",
    "dataset_iterator = iter(dataset_pipeline)\n",
    "dataset = next(dataset_iterator)\n",
    "\n",
    "print(f'Fetched {TARGET_PROBLEM} from {TARGET_DATASET}')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pypose as pp\n",
    "\n",
    "def trim_dataset(dataset, max_pixels):\n",
    "  trimmed_dataset = dict()\n",
    "  trimmed_dataset['points_2d'] = dataset['points_2d'][:max_pixels]\n",
    "  trimmed_dataset['point_index_of_observations'] = dataset['point_index_of_observations'][:max_pixels]\n",
    "  trimmed_dataset['camera_index_of_observations'] = dataset['camera_index_of_observations'][:max_pixels]\n",
    "  # other fields are not changed\n",
    "  trimmed_dataset['camera_extrinsics'] = dataset['camera_extrinsics']\n",
    "  trimmed_dataset['camera_intrinsics'] = dataset['camera_intrinsics']\n",
    "  trimmed_dataset['camera_distortions'] = dataset['camera_distortions']\n",
    "  trimmed_dataset['points_3d'] = dataset['points_3d']\n",
    "\n",
    "  for k in trimmed_dataset.keys():\n",
    "    if not isinstance(trimmed_dataset[k], torch.Tensor):\n",
    "      trimmed_dataset[k] = torch.from_numpy(trimmed_dataset[k])\n",
    "    trimmed_dataset[k] = trimmed_dataset[k].to(DEVICE)\n",
    "  return trimmed_dataset\n",
    "\n",
    "trimmed_dataset = trim_dataset(dataset, max_pixels=MAX_PIXELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojerr(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixels)\n",
    "\n",
    "  return loss\n",
    "    \n",
    "def reprojerr_vmap(pose, point, intrinsic, distortion, pixel):\n",
    "  # reprojerr_vmap is not batched, it operates on a single 3D point and camera\n",
    "  pose = pp.LieTensor(pose, ltype=pp.SE3_type) # pose will lose its ltype through vmap, temporary fix\n",
    "  point = pose.unsqueeze(-2) @ point\n",
    "  point = point.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  point_proj = -point[:2] / point[-1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  f = intrinsic[0, 0]\n",
    "  k1 = distortion[0]\n",
    "  k2 = distortion[1]\n",
    "  n = torch.sum(point_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f * r * point_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixel)\n",
    "\n",
    "  return loss\n",
    "\n",
    "def reprojerr_gen(*args):\n",
    "    if args[4].shape[0] == MAX_PIXELS:\n",
    "        return reprojerr(*args)\n",
    "    else:\n",
    "        args = list(args)\n",
    "        return reprojerr_vmap(*args[:-2])\n",
    "\n",
    "# sparse version\n",
    "class ReprojNonBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr_gen(self.pose, self.points_3d, *args)\n",
    "\n",
    "def least_square_error(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the least square error\n",
    "  return (torch.flatten(img_repj - pixels) ** 2).sum() / 2\n",
    "\n",
    "def mean_square_error(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the mean suqared error\n",
    "  return (img_repj - pixels).norm(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loss: 5.668307304382324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zitongzhan/Developer/pypose/pypose/optim/functional.py:173: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n",
      "  dummy_csc = dummy_coo.coalesce().to_sparse_csc()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addmm: computation on CPU is not implemented for Strided + SparseBsr @ Strided without MKL. PyTorch built with MKL has better support for addmm with sparse CPU tensors. \n",
      "Linear solver failed. Breaking optimization step...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "addmm: computation on CPU is not implemented for Strided + SparseBsr @ Strided without MKL. PyTorch built with MKL has better support for addmm with sparse CPU tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, mean_square_error(model_non_batched\u001b[38;5;241m.\u001b[39mpose, model_non_batched\u001b[38;5;241m.\u001b[39mpoints_3d, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_sparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeast square loss \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m @ \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(mean_square_error(model_non_batched\u001b[38;5;241m.\u001b[39mpose, model_non_batched\u001b[38;5;241m.\u001b[39mpoints_3d, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mitem(), idx))\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-5\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/pypose/pypose/optim/optimizer.py:568\u001b[0m, in \u001b[0;36mLevenbergMarquardt.step\u001b[0;34m(self, input, target, weight)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLinear solver failed. Breaking optimization step...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_parameter(pg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m], D)\n",
      "File \u001b[0;32m~/Developer/pypose/pypose/optim/optimizer.py:557\u001b[0m, in \u001b[0;36mLevenbergMarquardt.step\u001b[0;34m(self, input, target, weight)\u001b[0m\n\u001b[1;32m    555\u001b[0m     D \u001b[38;5;241m=\u001b[39m D\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_A:\n\u001b[0;32m--> 557\u001b[0m     D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver(A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64), b \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mJ_T\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64), x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_D)\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m#D = self.solver(A = A.to_sparse_csr(), b = (-J_T @ R.view(-1, 1)), x = self.prev_D)\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_D \u001b[38;5;241m=\u001b[39m D\n",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m, in \u001b[0;36m_sparse_csr_mm\u001b[0;34m(mat1, mat2)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sparse_csr_mm(mat1\u001b[38;5;241m.\u001b[39mto_sparse_csr(), mat2)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mat2\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmat1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmat1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmat1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmat2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39maddmm(\n\u001b[1;32m     66\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros([mat1\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), mat2\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)], dtype\u001b[38;5;241m=\u001b[39mmat1\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mmat1\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m     67\u001b[0m     layout\u001b[38;5;241m=\u001b[39mmat1\u001b[38;5;241m.\u001b[39mlayout),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     71\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: addmm: computation on CPU is not implemented for Strided + SparseBsr @ Strided without MKL. PyTorch built with MKL has better support for addmm with sparse CPU tensors."
     ]
    }
   ],
   "source": [
    "from pypose.optim.solver import CG\n",
    "input = [trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['points_2d'],\n",
    "         trimmed_dataset['camera_index_of_observations'],\n",
    "         trimmed_dataset['point_index_of_observations']]\n",
    "\n",
    "\n",
    "model_non_batched = ReprojNonBatched(trimmed_dataset['camera_extrinsics'].clone(),\n",
    "                                     trimmed_dataset['points_3d'].clone())\n",
    "\n",
    "model_non_batched = model_non_batched.to(DEVICE)\n",
    "\n",
    "strategy_sparse = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, solver=CG(tol=1e-2), reject=1, dense_A=False, scipy=False)\n",
    "\n",
    "print('Starting loss:', mean_square_error(model_non_batched.pose, model_non_batched.points_3d, *input).item())\n",
    "for idx in range(30):\n",
    "    loss = optimizer_sparse.step(input)\n",
    "    print('Least square loss %.3f @ %d it'%(mean_square_error(model_non_batched.pose, model_non_batched.points_3d, *input).item(), idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n",
    "print('Ending loss:', mean_square_error(model_non_batched.pose, model_non_batched.points_3d, *input).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
