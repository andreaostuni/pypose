{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5RoSZ7kJg-W3"
   },
   "source": [
    "# Bundle Adjustment Example using PyPose and the BAL dataset\n",
    "\n",
    "```\n",
    "The dataset is from the following paper:  \n",
    "Sameer Agarwal, Noah Snavely, Steven M. Seitz, and Richard Szeliski.  \n",
    "Bundle adjustment in the large.  \n",
    "In European Conference on Computer Vision (ECCV), 2010.  \n",
    "```\n",
    "\n",
    "Link to the dataset: https://grail.cs.washington.edu/projects/bal/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse-Jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data for ladybug...\n",
      "Fetched problem-49-7776-pre from ladybug\n"
     ]
    }
   ],
   "source": [
    "from bal_loader import build_pipeline\n",
    "\n",
    "TARGET_DATASET = \"ladybug\"\n",
    "TARGET_PROBLEM = \"problem-49-7776-pre\"\n",
    "\n",
    "def filter_problem(x):\n",
    "  return x['problem_name'] == TARGET_PROBLEM\n",
    "\n",
    "dataset_pipeline = build_pipeline(dataset=TARGET_DATASET, cache_dir='bal_data').filter(filter_problem)\n",
    "dataset_iterator = iter(dataset_pipeline)\n",
    "dataset = next(dataset_iterator)\n",
    "\n",
    "print(f'Fetched {TARGET_PROBLEM} from {TARGET_DATASET}')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pypose as pp\n",
    "\n",
    "# Load data\n",
    "MAX_PIXELS = 31843 # the full dataset has 30k pixels, and we trim a fraction of it\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "def trim_dataset(dataset, max_pixels):\n",
    "  trimmed_dataset = dict()\n",
    "  trimmed_dataset['points_2d'] = dataset['points_2d'][:max_pixels]\n",
    "  trimmed_dataset['point_index_of_observations'] = dataset['point_index_of_observations'][:max_pixels]\n",
    "  trimmed_dataset['camera_index_of_observations'] = dataset['camera_index_of_observations'][:max_pixels]\n",
    "  # other fields are not changed\n",
    "  trimmed_dataset['camera_extrinsics'] = dataset['camera_extrinsics']\n",
    "  trimmed_dataset['camera_intrinsics'] = dataset['camera_intrinsics']\n",
    "  trimmed_dataset['camera_distortions'] = dataset['camera_distortions']\n",
    "  trimmed_dataset['points_3d'] = dataset['points_3d']\n",
    "\n",
    "  for k in trimmed_dataset.keys():\n",
    "    if not isinstance(trimmed_dataset[k], torch.Tensor):\n",
    "      trimmed_dataset[k] = torch.from_numpy(trimmed_dataset[k])\n",
    "    trimmed_dataset[k] = trimmed_dataset[k].to(DEVICE)\n",
    "  return trimmed_dataset\n",
    "\n",
    "def reprojerr(pose, points, pixels, intrinsics, distortions, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  # intrinsics = intrinsics[camera_index]\n",
    "  # distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixels).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "trimmed_dataset = trim_dataset(dataset, max_pixels=MAX_PIXELS)\n",
    "\n",
    "# errors = reprojerr(trimmed_dataset['camera_extrinsics'], # [49, 7], a LieTensor\n",
    "#                    trimmed_dataset['points_3d'], # [7776, 3]\n",
    "#                    trimmed_dataset['points_2d'], # [1000, 2] before trimmed [31843, 2]\n",
    "#                    trimmed_dataset['camera_intrinsics'], # [49, 3, 3]\n",
    "#                    trimmed_dataset['camera_distortions'],) # [49, 2\n",
    "                  #  trimmed_dataset['point_index_of_observations'], # [1000] this 2-D point is produced by which 3-D point\n",
    "                  #  trimmed_dataset['camera_index_of_observations']) # [1000] this 2-D point is produced by which camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojerr_vmap(pose, point, pixel, intrinsic, distortion):\n",
    "  # reprojerr_vmap is not batched, it operates on a single 3D point and camera\n",
    "  pose = pp.LieTensor(pose, ltype=pp.SE3_type) # pose will lose its ltype through vmap, temporary fix\n",
    "  point = pose.unsqueeze(-2) @ point\n",
    "  point = point.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  point_proj = -point[:2] / point[-1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  f = intrinsic[0, 0]\n",
    "  k1 = distortion[0]\n",
    "  k2 = distortion[1]\n",
    "  n = torch.sum(point_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f * r * point_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixel).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "# jac_function_vmap = torch.vmap(pp.func.jacrev(reprojerr_vmap))\n",
    "# jac_from_vmap = jac_function_vmap(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "#                       trimmed_dataset['points_2d'],\n",
    "#                       trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']])\n",
    "# print(jac_from_vmap.shape)\n",
    "\n",
    "def reprojerr_gen(*args):\n",
    "    if args[2].shape[0] == MAX_PIXELS:\n",
    "        return reprojerr(*args)\n",
    "    else:\n",
    "        args = list(args)\n",
    "        return reprojerr_vmap(*args[:-2])\n",
    "\n",
    "# sparse version\n",
    "class ReprojNonBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d):\n",
    "        super().__init__()\n",
    "        self.pose = pp.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr_gen(self.pose, self.points_3d, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densed version\n",
    "class ReprojBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr(self.pose, self.points_3d, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypose.optim.solver import Krylov\n",
    "input = [trimmed_dataset['points_2d'],\n",
    "         trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "         trimmed_dataset['camera_index_of_observations'],\n",
    "         trimmed_dataset['point_index_of_observations']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huanxu/Desktop/current/github/pypose.nosync/pypose/optim/optimizer.py:521: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n",
      "  A, self.reject_count = J_T @ J, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matvec percentage: 98.48%\n",
      "accept step after 0 retries\n",
      "step time percentage: 91.83%\n",
      "Pose Inversion loss 1023718.3125000 @ 0 it\n",
      "matvec percentage: 97.48%\n",
      "accept step after 0 retries\n",
      "step time percentage: 91.13%\n",
      "Pose Inversion loss 545104.0625000 @ 1 it\n",
      "matvec percentage: 98.67%\n",
      "reject step 1\n",
      "matvec percentage: 98.86%\n",
      "accept step after 1 retries\n",
      "step time percentage: 98.78%\n",
      "Pose Inversion loss 678954.8750000 @ 2 it\n",
      "matvec percentage: 98.19%\n",
      "accept step after 0 retries\n",
      "step time percentage: 95.40%\n",
      "Pose Inversion loss 663485.5000000 @ 3 it\n",
      "matvec percentage: 97.02%\n",
      "accept step after 0 retries\n",
      "step time percentage: 86.15%\n",
      "Pose Inversion loss 501983.0937500 @ 4 it\n",
      "matvec percentage: 98.03%\n",
      "accept step after 0 retries\n",
      "step time percentage: 95.01%\n",
      "Pose Inversion loss 474464.6562500 @ 5 it\n",
      "matvec percentage: 98.64%\n",
      "reject step 1\n",
      "matvec percentage: 98.83%\n",
      "accept step after 1 retries\n",
      "step time percentage: 98.67%\n",
      "Pose Inversion loss 603574.8750000 @ 6 it\n",
      "matvec percentage: 98.32%\n",
      "accept step after 0 retries\n",
      "step time percentage: 95.26%\n",
      "Pose Inversion loss 490914.9062500 @ 7 it\n",
      "matvec percentage: 98.35%\n",
      "reject step 1\n",
      "matvec percentage: 98.55%\n",
      "accept step after 1 retries\n",
      "step time percentage: 98.19%\n",
      "Pose Inversion loss 517272.5625000 @ 8 it\n",
      "matvec percentage: 97.36%\n",
      "accept step after 0 retries\n",
      "step time percentage: 90.20%\n",
      "Pose Inversion loss 460538.8437500 @ 9 it\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "model_non_batched = ReprojNonBatched(trimmed_dataset['camera_extrinsics'].clone(),\n",
    "                                     trimmed_dataset['points_3d'].clone())\n",
    "\n",
    "model_non_batched = model_non_batched.to(DEVICE)\n",
    "\n",
    "strategy_sparse = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, solver=Krylov(rtol=5e-2, debug=True), reject=1, debug=True)\n",
    "for idx in range(10):\n",
    "    loss = optimizer_sparse.step(input)\n",
    "    # if (idx + 1) % 10 == 0:\n",
    "    print('Pose Inversion loss %.7f @ %d it'%(loss, idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "model_batched = ReprojBatched(trimmed_dataset['camera_extrinsics'].clone(), trimmed_dataset['points_3d'].clone())\n",
    "\n",
    "strategy_dense = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_dense = pp.optim.LM(model_batched, strategy=strategy_dense, sparse=False)\n",
    "for idx in range(3):\n",
    "    loss = optimizer_dense.step(input)\n",
    "    print('Pose Inversion loss %.7f @ %d it'%(loss, idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct SBT from vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "v = torch.arange(12).view((-1, 1, 3)).to(dtype=torch.float32)\n",
    "x = pp.sbktensor.sbktensor(i, v, size=(3, 3), dtype=torch.float32)\n",
    "print(x.to_sparse_coo().shape)\n",
    "x2 = torch.arange(27).view(9, 3).to(dtype=torch.float32)\n",
    "x = torch.sparse_coo_tensor(i, v, size=(3, 3), dtype=torch.float32)\n",
    "print(x.to_sparse_coo().to_dense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "# v = torch.randn(4 * 2 * 2).view((4, 2, 2)).to(dtype=torch.float32)\n",
    "v = torch.randn(4)\n",
    "sct = torch.sparse_coo_tensor(i, v)\n",
    "v2 = torch.randn(4)\n",
    "sct2 = torch.sparse_coo_tensor(i, v2)\n",
    "# torch.cat([sct, sct2], dim=1)\n",
    "# pdt = torch.sparse.mm(v, v2)\n",
    "pdt = sct @ sct2.T\n",
    "print(pdt.type())\n",
    "print(pdt.is_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the jacobian values from jacrev and vmap match\n",
    "batch_dim = jac_from_jacrev.shape[0]\n",
    "index = trimmed_dataset['camera_index_of_observations']\n",
    "jac_from_jacrev_condensed = jac_from_jacrev[torch.arange(batch_dim), index]\n",
    "print(jac_from_jacrev_condensed.shape)\n",
    "if torch.allclose(jac_from_vmap, jac_from_jacrev_condensed):\n",
    "  print(\"Jacobian structure sanity check: ok!\")\n",
    "else:\n",
    "  print(\"Jacobian structure sanity check: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually construct SBT from vmap result\n",
    "import torch\n",
    "import pypose as pp\n",
    "from pypose.sparse import sbktensor\n",
    "\n",
    "def construct_sbt(jac_from_vmap, num_cameras, camera_index):\n",
    "  # camera_index = torch.from_numpy(camera_index) # for torch.stack\n",
    "  n = camera_index.shape[0] # num 2D points\n",
    "  # i = torch.stack([torch.arange(n), camera_index, torch.zeros(n)])\n",
    "  i = torch.stack([torch.arange(n), camera_index])\n",
    "  print(i.shape)\n",
    "  # v = jac_from_vmap[:, None, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  v = jac_from_vmap[:, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  # return pp.sbktensor(i, v, size=(n, num_cameras, 1), dtype=torch.float32)\n",
    "  return pp.sbktensor.sbktensor(i, v, size=(n, num_cameras), dtype=torch.float32)\n",
    "\n",
    "sparse_jac = construct_sbt(jac_from_vmap, len(trimmed_dataset['camera_extrinsics']), trimmed_dataset['camera_index_of_observations'])\n",
    "dense_jac = sparse_jac.to_dense()\n",
    "# if torch.allclose(jac_from_jacrev, dense_jac):\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: ok!\")\n",
    "# else:\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_t = sparse_jac._s\n",
    "print(f'shape: {storage_t.shape}, dense dim: {storage_t.dense_dim()}, \\\n",
    "sparse: {storage_t.sparse_dim()}')\n",
    "flattened_coo = pp.hybrid2coo(storage_t) # the flattened tensor\n",
    "print(flattened_coo.shape)\n",
    "flattened_coo_T = flattened_coo.T\n",
    "print(flattened_coo_T.shape)\n",
    "A = torch.sparse.mm(flattened_coo_T, flattened_coo)\n",
    "print(A.shape)\n",
    "print(A.type())\n",
    "# pdt = torch.matmul(flattened_coo_T, torch.arange(1000).view(-1, 1).to(dtype=torch.float32))\n",
    "# print(pdt.shape)\n",
    "# print(pdt.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_coo_diagonal(t: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    return t.values()[diag_indices]\n",
    "\n",
    "def sparse_coo_diagonal_clamp_(t: torch.Tensor, min_value, max_value):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].clamp_(min_value, max_value)\n",
    "\n",
    "\n",
    "def sparse_coo_diagonal_add_(t: torch.Tensor, other: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].add_(other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_indices = (A.indices()[0] == A.indices()[1]).nonzero(as_tuple=True)\n",
    "# print(diag_indices[0][])\n",
    "sparse_coo_diagonal_clamp_(A, -100, 100)\n",
    "print(A.values()[diag_indices[0][:10]])\n",
    "sparse_coo_diagonal_add_(A, sparse_coo_diagonal(A) * torch.ones(301) * 2)\n",
    "print(A.values()[diag_indices[0][:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacrev_custom(func, argnums):\n",
    "  def wrapper(*args, **kwargs):\n",
    "    jac_vmap = torch.vmap(pp.func.jacrev(func)) # vmap\n",
    "    gradients = jac_vmap(*args)\n",
    "    sbt = construct_sbt(gradients, kwargs['num_cols'], kwargs['index'])\n",
    "    return sbt\n",
    "  return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_function_custom = jacrev_custom(reprojerr_vmap, argnums=0)\n",
    "jac_from_custom = jac_function_custom(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "                      trimmed_dataset['points_2d'],\n",
    "                      trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      num_cols=len(trimmed_dataset['camera_extrinsics']),\n",
    "                      index=trimmed_dataset['camera_index_of_observations'])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
