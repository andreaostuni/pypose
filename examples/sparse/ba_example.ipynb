{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5RoSZ7kJg-W3"
   },
   "source": [
    "# Bundle Adjustment Example using PyPose and the BAL dataset\n",
    "\n",
    "```\n",
    "The dataset is from the following paper:  \n",
    "Sameer Agarwal, Noah Snavely, Steven M. Seitz, and Richard Szeliski.  \n",
    "Bundle adjustment in the large.  \n",
    "In European Conference on Computer Vision (ECCV), 2010.  \n",
    "```\n",
    "\n",
    "Link to the dataset: https://grail.cs.washington.edu/projects/bal/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse-Jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bal_loader import build_pipeline\n",
    "\n",
    "# TARGET_DATASET = \"trafalgar\"\n",
    "# TARGET_PROBLEM = \"problem-257-65132-pre\"\n",
    "# MAX_PIXELS = 225911\n",
    "\n",
    "TARGET_DATASET = \"ladybug\"\n",
    "TARGET_PROBLEM = \"problem-49-7776-pre\"\n",
    "MAX_PIXELS = 31843\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "def filter_problem(x):\n",
    "  return x['problem_name'] == TARGET_PROBLEM\n",
    "\n",
    "dataset_pipeline = build_pipeline(dataset=TARGET_DATASET, cache_dir='bal_data').filter(filter_problem)\n",
    "dataset_iterator = iter(dataset_pipeline)\n",
    "dataset = next(dataset_iterator)\n",
    "\n",
    "print(f'Fetched {TARGET_PROBLEM} from {TARGET_DATASET}')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pypose as pp\n",
    "\n",
    "def trim_dataset(dataset, max_pixels):\n",
    "  trimmed_dataset = dict()\n",
    "  trimmed_dataset['points_2d'] = dataset['points_2d'][:max_pixels]\n",
    "  trimmed_dataset['point_index_of_observations'] = dataset['point_index_of_observations'][:max_pixels]\n",
    "  trimmed_dataset['camera_index_of_observations'] = dataset['camera_index_of_observations'][:max_pixels]\n",
    "  # other fields are not changed\n",
    "  trimmed_dataset['camera_extrinsics'] = dataset['camera_extrinsics']\n",
    "  trimmed_dataset['camera_intrinsics'] = dataset['camera_intrinsics']\n",
    "  trimmed_dataset['camera_distortions'] = dataset['camera_distortions']\n",
    "  trimmed_dataset['points_3d'] = dataset['points_3d']\n",
    "\n",
    "  for k in trimmed_dataset.keys():\n",
    "    if not isinstance(trimmed_dataset[k], torch.Tensor):\n",
    "      trimmed_dataset[k] = torch.from_numpy(trimmed_dataset[k])\n",
    "    trimmed_dataset[k] = trimmed_dataset[k].to(DEVICE)\n",
    "  return trimmed_dataset\n",
    "\n",
    "def reprojerr(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixels).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "trimmed_dataset = trim_dataset(dataset, max_pixels=MAX_PIXELS)\n",
    "\n",
    "# errors = reprojerr(trimmed_dataset['camera_extrinsics'], # [49, 7], a LieTensor\n",
    "#                    trimmed_dataset['points_3d'], # [7776, 3]\n",
    "#                    trimmed_dataset['points_2d'], # [1000, 2] before trimmed [31843, 2]\n",
    "#                    trimmed_dataset['camera_intrinsics'], # [49, 3, 3]\n",
    "#                    trimmed_dataset['camera_distortions'],) # [49, 2\n",
    "                  #  trimmed_dataset['point_index_of_observations'], # [1000] this 2-D point is produced by which 3-D point\n",
    "                  #  trimmed_dataset['camera_index_of_observations']) # [1000] this 2-D point is produced by which camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojerr_vmap(pose, point, intrinsic, distortion, pixel):\n",
    "  # reprojerr_vmap is not batched, it operates on a single 3D point and camera\n",
    "  pose = pp.LieTensor(pose, ltype=pp.SE3_type) # pose will lose its ltype through vmap, temporary fix\n",
    "  point = pose.unsqueeze(-2) @ point\n",
    "  point = point.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  point_proj = -point[:2] / point[-1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  f = intrinsic[0, 0]\n",
    "  k1 = distortion[0]\n",
    "  k2 = distortion[1]\n",
    "  n = torch.sum(point_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f * r * point_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixel).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "# jac_function_vmap = torch.vmap(pp.func.jacrev(reprojerr_vmap))\n",
    "# jac_from_vmap = jac_function_vmap(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "#                       trimmed_dataset['points_2d'],\n",
    "#                       trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']])\n",
    "# print(jac_from_vmap.shape)\n",
    "\n",
    "def reprojerr_gen(*args):\n",
    "    if args[4].shape[0] == MAX_PIXELS:\n",
    "        return reprojerr(*args)\n",
    "    else:\n",
    "        args = list(args)\n",
    "        return reprojerr_vmap(*args[:-2])\n",
    "\n",
    "# sparse version\n",
    "class ReprojNonBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d, camera_intrinsics, camera_distortions):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "        self.intrinsics = nn.Parameter(camera_intrinsics)\n",
    "        self.distortions = nn.Parameter(camera_distortions)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr_gen(self.pose, self.points_3d, self.intrinsics, self.distortions, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densed version\n",
    "class ReprojBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d, camera_intrinsics):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "        self.intrinsics = nn.Parameter(camera_intrinsics)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr(self.pose, self.points_3d, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypose.optim.solver import Krylov\n",
    "input = [trimmed_dataset['points_2d'],\n",
    "         trimmed_dataset['camera_index_of_observations'],\n",
    "         trimmed_dataset['point_index_of_observations']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square_error(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the least square error\n",
    "  return (torch.flatten(img_repj - pixels) ** 2).sum() / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_non_batched = ReprojNonBatched(trimmed_dataset['camera_extrinsics'].clone(),\n",
    "                                     trimmed_dataset['points_3d'].clone(),\n",
    "                                     trimmed_dataset['camera_intrinsics'].clone(),\n",
    "                                     trimmed_dataset['camera_distortions'].clone())\n",
    "\n",
    "model_non_batched = model_non_batched.to(DEVICE)\n",
    "\n",
    "strategy_sparse = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, solver=Krylov(rtol=3e-4, iterations=5000), reject=1)\n",
    "# optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, dense_A=True)\n",
    "\n",
    "print('starting loss:', least_square_error(model_non_batched.pose, model_non_batched.points_3d, model_non_batched.intrinsics, model_non_batched.distortions, *input).item())\n",
    "for idx in range(20):\n",
    "    loss = optimizer_sparse.step(input)\n",
    "    # if (idx + 1) % 10 == 0:\n",
    "    print('Least square loss %.3f @ %d it'%(least_square_error(model_non_batched.pose, model_non_batched.points_3d, model_non_batched.intrinsics, model_non_batched.distortions, *input).item(), idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n",
    "print('ending loss:', least_square_error(model_non_batched.pose, model_non_batched.points_3d, model_non_batched.intrinsics, model_non_batched.distortions, *input).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(model_non_batched.named_parameters())\n",
    "points_3d_opt = params['points_3d'].to(device='cpu').detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('points_3d_opt_problem-257-65132-pre.npy', 'wb') as f:\n",
    "    np.save(f, points_3d_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_batched = ReprojBatched(trimmed_dataset['camera_extrinsics'].clone(), trimmed_dataset['points_3d'].clone())\n",
    "\n",
    "strategy_dense = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_dense = pp.optim.LM(model_batched, strategy=strategy_dense, sparse=False)\n",
    "for idx in range(3):\n",
    "    loss = optimizer_dense.step(input)\n",
    "    print('Pose Inversion loss %.7f @ %d it'%(loss, idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct SBT from vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypose as pp\n",
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "v = torch.arange(12).view((-1, 1, 3)).to(dtype=torch.float32)\n",
    "x = pp.sbktensor(i, v, size=(3, 3), dtype=torch.float32)\n",
    "storage_t = x._s\n",
    "\n",
    "print(f'shape: {storage_t.shape}, dense dim: {storage_t.dense_dim()}, \\\n",
    "sparse: {storage_t.sparse_dim()}')\n",
    "flattened_coo = pp.hybrid2coo(storage_t) # the flattened tensor\n",
    "print(flattened_coo.shape)\n",
    "flattened_coo_T = flattened_coo.T\n",
    "print(flattened_coo_T.shape)\n",
    "A = torch.sparse.mm(flattened_coo_T, flattened_coo)\n",
    "print(A.shape)\n",
    "print(A.type())\n",
    "# pdt = torch.matmul(flattened_coo_T, torch.arange(1000).view(-1, 1).to(dtype=torch.float32))\n",
    "# print(pdt.shape)\n",
    "# print(pdt.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.to_sparse_csr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "# v = torch.randn(4 * 2 * 2).view((4, 2, 2)).to(dtype=torch.float32)\n",
    "v = torch.randn(4)\n",
    "sct = torch.sparse_coo_tensor(i, v)\n",
    "v2 = torch.randn(4)\n",
    "sct2 = torch.sparse_coo_tensor(i, v2)\n",
    "# torch.cat([sct, sct2], dim=1)\n",
    "# pdt = torch.sparse.mm(v, v2)\n",
    "pdt = sct @ sct2.T\n",
    "print(pdt.type())\n",
    "print(pdt.is_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the jacobian values from jacrev and vmap match\n",
    "batch_dim = jac_from_jacrev.shape[0]\n",
    "index = trimmed_dataset['camera_index_of_observations']\n",
    "jac_from_jacrev_condensed = jac_from_jacrev[torch.arange(batch_dim), index]\n",
    "print(jac_from_jacrev_condensed.shape)\n",
    "if torch.allclose(jac_from_vmap, jac_from_jacrev_condensed):\n",
    "  print(\"Jacobian structure sanity check: ok!\")\n",
    "else:\n",
    "  print(\"Jacobian structure sanity check: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually construct SBT from vmap result\n",
    "import torch\n",
    "import pypose as pp\n",
    "from pypose.sparse import sbktensor\n",
    "\n",
    "def construct_sbt(jac_from_vmap, num_cameras, camera_index):\n",
    "  # camera_index = torch.from_numpy(camera_index) # for torch.stack\n",
    "  n = camera_index.shape[0] # num 2D points\n",
    "  # i = torch.stack([torch.arange(n), camera_index, torch.zeros(n)])\n",
    "  i = torch.stack([torch.arange(n), camera_index])\n",
    "  print(i.shape)\n",
    "  # v = jac_from_vmap[:, None, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  v = jac_from_vmap[:, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  # return pp.sbktensor(i, v, size=(n, num_cameras, 1), dtype=torch.float32)\n",
    "  return pp.sbktensor.sbktensor(i, v, size=(n, num_cameras), dtype=torch.float32)\n",
    "\n",
    "sparse_jac = construct_sbt(jac_from_vmap, len(trimmed_dataset['camera_extrinsics']), trimmed_dataset['camera_index_of_observations'])\n",
    "dense_jac = sparse_jac.to_dense()\n",
    "# if torch.allclose(jac_from_jacrev, dense_jac):\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: ok!\")\n",
    "# else:\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_t = sparse_jac._s\n",
    "print(f'shape: {storage_t.shape}, dense dim: {storage_t.dense_dim()}, \\\n",
    "sparse: {storage_t.sparse_dim()}')\n",
    "flattened_coo = pp.hybrid2coo(storage_t) # the flattened tensor\n",
    "print(flattened_coo.shape)\n",
    "flattened_coo_T = flattened_coo.T\n",
    "print(flattened_coo_T.shape)\n",
    "A = torch.sparse.mm(flattened_coo_T, flattened_coo)\n",
    "print(A.shape)\n",
    "print(A.type())\n",
    "# pdt = torch.matmul(flattened_coo_T, torch.arange(1000).view(-1, 1).to(dtype=torch.float32))\n",
    "# print(pdt.shape)\n",
    "# print(pdt.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_coo_diagonal(t: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    return t.values()[diag_indices]\n",
    "\n",
    "def sparse_coo_diagonal_clamp_(t: torch.Tensor, min_value, max_value):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].clamp_(min_value, max_value)\n",
    "\n",
    "\n",
    "def sparse_coo_diagonal_add_(t: torch.Tensor, other: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].add_(other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_indices = (A.indices()[0] == A.indices()[1]).nonzero(as_tuple=True)\n",
    "# print(diag_indices[0][])\n",
    "sparse_coo_diagonal_clamp_(A, -100, 100)\n",
    "print(A.values()[diag_indices[0][:10]])\n",
    "sparse_coo_diagonal_add_(A, sparse_coo_diagonal(A) * torch.ones(301) * 2)\n",
    "print(A.values()[diag_indices[0][:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacrev_custom(func, argnums):\n",
    "  def wrapper(*args, **kwargs):\n",
    "    jac_vmap = torch.vmap(pp.func.jacrev(func)) # vmap\n",
    "    gradients = jac_vmap(*args)\n",
    "    sbt = construct_sbt(gradients, kwargs['num_cols'], kwargs['index'])\n",
    "    return sbt\n",
    "  return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_function_custom = jacrev_custom(reprojerr_vmap, argnums=0)\n",
    "jac_from_custom = jac_function_custom(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "                      trimmed_dataset['points_2d'],\n",
    "                      trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      num_cols=len(trimmed_dataset['camera_extrinsics']),\n",
    "                      index=trimmed_dataset['camera_index_of_observations'])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pypose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
