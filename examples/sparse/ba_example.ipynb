{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5RoSZ7kJg-W3"
   },
   "source": [
    "# Bundle Adjustment Example using PyPose and the BAL dataset\n",
    "\n",
    "```\n",
    "The dataset is from the following paper:  \n",
    "Sameer Agarwal, Noah Snavely, Steven M. Seitz, and Richard Szeliski.  \n",
    "Bundle adjustment in the large.  \n",
    "In European Conference on Computer Vision (ECCV), 2010.  \n",
    "```\n",
    "\n",
    "Link to the dataset: https://grail.cs.washington.edu/projects/bal/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse-Jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data for ladybug...\n",
      "Fetched problem-49-7776-pre from ladybug\n"
     ]
    }
   ],
   "source": [
    "from bal_loader import build_pipeline\n",
    "\n",
    "# TARGET_DATASET = \"trafalgar\"\n",
    "# TARGET_PROBLEM = \"problem-257-65132-pre\"\n",
    "# MAX_PIXELS = 225911\n",
    "\n",
    "TARGET_DATASET = \"ladybug\"\n",
    "TARGET_PROBLEM = \"problem-49-7776-pre\"\n",
    "MAX_PIXELS = 31843\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "def filter_problem(x):\n",
    "  return x['problem_name'] == TARGET_PROBLEM\n",
    "\n",
    "dataset_pipeline = build_pipeline(dataset=TARGET_DATASET, cache_dir='bal_data').filter(filter_problem)\n",
    "dataset_iterator = iter(dataset_pipeline)\n",
    "dataset = next(dataset_iterator)\n",
    "\n",
    "print(f'Fetched {TARGET_PROBLEM} from {TARGET_DATASET}')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pypose as pp\n",
    "\n",
    "def trim_dataset(dataset, max_pixels):\n",
    "  trimmed_dataset = dict()\n",
    "  trimmed_dataset['points_2d'] = dataset['points_2d'][:max_pixels]\n",
    "  trimmed_dataset['point_index_of_observations'] = dataset['point_index_of_observations'][:max_pixels]\n",
    "  trimmed_dataset['camera_index_of_observations'] = dataset['camera_index_of_observations'][:max_pixels]\n",
    "  # other fields are not changed\n",
    "  trimmed_dataset['camera_extrinsics'] = dataset['camera_extrinsics']\n",
    "  trimmed_dataset['camera_intrinsics'] = dataset['camera_intrinsics']\n",
    "  trimmed_dataset['camera_distortions'] = dataset['camera_distortions']\n",
    "  trimmed_dataset['points_3d'] = dataset['points_3d']\n",
    "\n",
    "  for k in trimmed_dataset.keys():\n",
    "    if not isinstance(trimmed_dataset[k], torch.Tensor):\n",
    "      trimmed_dataset[k] = torch.from_numpy(trimmed_dataset[k])\n",
    "    trimmed_dataset[k] = trimmed_dataset[k].to(DEVICE)\n",
    "  return trimmed_dataset\n",
    "\n",
    "def reprojerr(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixels).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "trimmed_dataset = trim_dataset(dataset, max_pixels=MAX_PIXELS)\n",
    "\n",
    "# errors = reprojerr(trimmed_dataset['camera_extrinsics'], # [49, 7], a LieTensor\n",
    "#                    trimmed_dataset['points_3d'], # [7776, 3]\n",
    "#                    trimmed_dataset['points_2d'], # [1000, 2] before trimmed [31843, 2]\n",
    "#                    trimmed_dataset['camera_intrinsics'], # [49, 3, 3]\n",
    "#                    trimmed_dataset['camera_distortions'],) # [49, 2\n",
    "                  #  trimmed_dataset['point_index_of_observations'], # [1000] this 2-D point is produced by which 3-D point\n",
    "                  #  trimmed_dataset['camera_index_of_observations']) # [1000] this 2-D point is produced by which camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojerr_vmap(pose, point, intrinsic, distortion, pixel):\n",
    "  # reprojerr_vmap is not batched, it operates on a single 3D point and camera\n",
    "  pose = pp.LieTensor(pose, ltype=pp.SE3_type) # pose will lose its ltype through vmap, temporary fix\n",
    "  point = pose.unsqueeze(-2) @ point\n",
    "  point = point.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  point_proj = -point[:2] / point[-1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  f = intrinsic[0, 0]\n",
    "  k1 = distortion[0]\n",
    "  k2 = distortion[1]\n",
    "  n = torch.sum(point_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f * r * point_proj\n",
    "\n",
    "  # calculate the reprojection error\n",
    "  loss = (img_repj - pixel).norm(dim=-1)\n",
    "\n",
    "  return loss\n",
    "\n",
    "# jac_function_vmap = torch.vmap(pp.func.jacrev(reprojerr_vmap))\n",
    "# jac_from_vmap = jac_function_vmap(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "#                       trimmed_dataset['points_2d'],\n",
    "#                       trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "#                       trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']])\n",
    "# print(jac_from_vmap.shape)\n",
    "\n",
    "def reprojerr_gen(*args):\n",
    "    if args[4].shape[0] == MAX_PIXELS:\n",
    "        return reprojerr(*args)\n",
    "    else:\n",
    "        args = list(args)\n",
    "        return reprojerr_vmap(*args[:-2])\n",
    "\n",
    "# sparse version\n",
    "class ReprojNonBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d, camera_intrinsics, camera_distortions):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "        self.intrinsics = nn.Parameter(camera_intrinsics)\n",
    "        self.distortions = nn.Parameter(camera_distortions)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr_gen(self.pose, self.points_3d, self.intrinsics, self.distortions, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densed version\n",
    "class ReprojBatched(nn.Module):\n",
    "    def __init__(self, camera_extrinsics, points_3d, camera_intrinsics):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Parameter(camera_extrinsics)\n",
    "        self.points_3d = nn.Parameter(points_3d)\n",
    "        self.intrinsics = nn.Parameter(camera_intrinsics)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return reprojerr(self.pose, self.points_3d, *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypose.optim.solver import CG\n",
    "input = [trimmed_dataset['points_2d'],\n",
    "         trimmed_dataset['camera_index_of_observations'],\n",
    "         trimmed_dataset['point_index_of_observations']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square_error(pose, points, intrinsics, distortions, pixels, camera_index, point_index):\n",
    "  points = points[point_index, None] # [1000, 1, 3]\n",
    "  pose = pose[camera_index] # [1000, 7]\n",
    "  points = pose.unsqueeze(-2) @ points\n",
    "  points = points.squeeze(-2)\n",
    "\n",
    "  # perspective division\n",
    "  points_proj = -points[:, :2] / points[:, -1:]\n",
    "\n",
    "  # convert to pixel coordinates\n",
    "  intrinsics = intrinsics[camera_index]\n",
    "  distortions = distortions[camera_index]\n",
    "  f = intrinsics[:, 0, 0]\n",
    "  k1 = distortions[:, 0]\n",
    "  k2 = distortions[:, 1]\n",
    "  n = torch.sum(points_proj**2, dim=-1)\n",
    "  r = 1.0 + k1 * n + k2 * n**2\n",
    "  img_repj = f[:, None] * r[:, None] * points_proj\n",
    "\n",
    "  # calculate the least square error\n",
    "  return (torch.flatten(img_repj - pixels) ** 2).sum() / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loss: 850912.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huanxu/Desktop/current/github/pypose.nosync/pypose/optim/optimizer.py:529: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n",
      "  A, self.reject_count = J_T @ J, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least square loss 83565.781 @ 0 it\n",
      "Least square loss 75347.578 @ 1 it\n",
      "Least square loss 56149.250 @ 2 it\n",
      "Least square loss 54615.961 @ 3 it\n",
      "Least square loss 52625.117 @ 4 it\n",
      "Least square loss 51664.199 @ 5 it\n",
      "Least square loss 41472.836 @ 6 it\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:14\u001b[0m\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/current/github/pypose.nosync/pypose/optim/optimizer.py:547\u001b[0m, in \u001b[0;36mLevenbergMarquardt.step\u001b[0;34m(self, input, target, weight)\u001b[0m\n\u001b[1;32m    545\u001b[0m A_indices \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mindices()\n\u001b[1;32m    546\u001b[0m A_scipy \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcoo_matrix((A\u001b[39m.\u001b[39mvalues()\u001b[39m.\u001b[39mnumpy(), (A_indices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy(), A_indices[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mnumpy())))\n\u001b[0;32m--> 547\u001b[0m D_scipy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver(A_scipy, (\u001b[39m-\u001b[39;49mJ_T \u001b[39m@\u001b[39;49m R\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mnumpy(), x0 \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprev_D, tol\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    548\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev_D \u001b[39m=\u001b[39m D_scipy\n\u001b[1;32m    549\u001b[0m D \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(D_scipy)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m<decorator-gen-3>:2\u001b[0m, in \u001b[0;36mcg\u001b[0;34m(A, b, x0, tol, maxiter, M, callback, atol)\u001b[0m\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/_lib/_threadsafety.py:44\u001b[0m, in \u001b[0;36mReentrancyLock.decorate.<locals>.caller\u001b[0;34m(func, *a, **kw)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcaller\u001b[39m(func, \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m     43\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/sparse/linalg/_isolve/iterative.py:329\u001b[0m, in \u001b[0;36mcg\u001b[0;34m(A, b, x0, tol, maxiter, M, callback, atol)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39melif\u001b[39;00m (ijob \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    328\u001b[0m     work[slice2] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m sclr2\n\u001b[0;32m--> 329\u001b[0m     work[slice2] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sclr1\u001b[39m*\u001b[39mmatvec(work[slice1])\n\u001b[1;32m    330\u001b[0m \u001b[39melif\u001b[39;00m (ijob \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m    331\u001b[0m     work[slice1] \u001b[39m=\u001b[39m psolve(work[slice2])\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/sparse/linalg/_interface.py:232\u001b[0m, in \u001b[0;36mLinearOperator.matvec\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (N,) \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (N,\u001b[39m1\u001b[39m):\n\u001b[1;32m    230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdimension mismatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_matvec(x)\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, np\u001b[39m.\u001b[39mmatrix):\n\u001b[1;32m    235\u001b[0m     y \u001b[39m=\u001b[39m asmatrix(y)\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/sparse/linalg/_interface.py:199\u001b[0m, in \u001b[0;36mLinearOperator._matvec\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_matvec\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    190\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Default matrix-vector multiplication handler.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[39m    If self is a linear operator of shape (M, N), then this method will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39m    will define matrix-vector multiplication as well.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatmat(x\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/sparse/linalg/_interface.py:337\u001b[0m, in \u001b[0;36mLinearOperator.matmat\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdimension mismatch: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    335\u001b[0m                      \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, X\u001b[39m.\u001b[39mshape))\n\u001b[0;32m--> 337\u001b[0m Y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_matmat(X)\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Y, np\u001b[39m.\u001b[39mmatrix):\n\u001b[1;32m    340\u001b[0m     Y \u001b[39m=\u001b[39m asmatrix(Y)\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/sparse/linalg/_interface.py:731\u001b[0m, in \u001b[0;36mMatrixLinearOperator._matmat\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_matmat\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m--> 731\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mA\u001b[39m.\u001b[39;49mdot(X)\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/sparse/_base.py:413\u001b[0m, in \u001b[0;36mspmatrix.dot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdot\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m    401\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Ordinary dot product\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \n\u001b[1;32m    403\u001b[0m \u001b[39m    Examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \n\u001b[1;32m    412\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m \u001b[39m@\u001b[39;49m other\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/sparse/_base.py:620\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[1;32m    618\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    619\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/sparse/_base.py:523\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_vector(other)\n\u001b[1;32m    522\u001b[0m \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (N, \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 523\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_vector(other\u001b[39m.\u001b[39;49mravel())\u001b[39m.\u001b[39mreshape(M, \u001b[39m1\u001b[39m)\n\u001b[1;32m    524\u001b[0m \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m N:\n\u001b[1;32m    525\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_multivector(other)\n",
      "File \u001b[0;32m~/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/scipy/sparse/_coo.py:578\u001b[0m, in \u001b[0;36mcoo_matrix._mul_vector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mul_vector\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m    575\u001b[0m     \u001b[39m#output array\u001b[39;00m\n\u001b[1;32m    576\u001b[0m     result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mupcast_char(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mchar,\n\u001b[1;32m    577\u001b[0m                                                         other\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mchar))\n\u001b[0;32m--> 578\u001b[0m     coo_matvec(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnnz, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrow, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcol, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, other, result)\n\u001b[1;32m    579\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_non_batched = ReprojNonBatched(trimmed_dataset['camera_extrinsics'].clone(),\n",
    "                                     trimmed_dataset['points_3d'].clone(),\n",
    "                                     trimmed_dataset['camera_intrinsics'].clone(),\n",
    "                                     trimmed_dataset['camera_distortions'].clone())\n",
    "\n",
    "model_non_batched = model_non_batched.to(DEVICE)\n",
    "\n",
    "strategy_sparse = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, solver=CG(maxiter=10000), reject=1, dense_A=False)\n",
    "#optimizer_sparse = pp.optim.LM(model_non_batched, strategy=strategy_sparse, dense_A=False, scipy=True)\n",
    "\n",
    "print('starting loss:', least_square_error(model_non_batched.pose, model_non_batched.points_3d, model_non_batched.intrinsics, model_non_batched.distortions, *input).item())\n",
    "for idx in range(20):\n",
    "    loss = optimizer_sparse.step(input)\n",
    "    # if (idx + 1) % 10 == 0:\n",
    "    print('Least square loss %.3f @ %d it'%(least_square_error(model_non_batched.pose, model_non_batched.points_3d, model_non_batched.intrinsics, model_non_batched.distortions, *input).item(), idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n",
    "print('ending loss:', least_square_error(model_non_batched.pose, model_non_batched.points_3d, model_non_batched.intrinsics, model_non_batched.distortions, *input).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(model_non_batched.named_parameters())\n",
    "points_3d_opt = params['points_3d'].to(device='cpu').detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('points_3d_opt_problem-257-65132-pre.npy', 'wb') as f:\n",
    "    np.save(f, points_3d_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_batched = ReprojBatched(trimmed_dataset['camera_extrinsics'].clone(), trimmed_dataset['points_3d'].clone())\n",
    "\n",
    "strategy_dense = pp.optim.strategy.Adaptive(damping=1e-6)\n",
    "optimizer_dense = pp.optim.LM(model_batched, strategy=strategy_dense, sparse=False)\n",
    "for idx in range(3):\n",
    "    loss = optimizer_dense.step(input)\n",
    "    print('Pose Inversion loss %.7f @ %d it'%(loss, idx))\n",
    "    if loss < 1e-5:\n",
    "        print('Early Stopping with loss:', loss.item())\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct SBT from vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypose as pp\n",
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "v = torch.arange(12).view((-1, 1, 3)).to(dtype=torch.float32)\n",
    "x = pp.sbktensor(i, v, size=(3, 3), dtype=torch.float32)\n",
    "storage_t = x._s\n",
    "\n",
    "print(f'shape: {storage_t.shape}, dense dim: {storage_t.dense_dim()}, \\\n",
    "sparse: {storage_t.sparse_dim()}')\n",
    "flattened_coo = pp.hybrid2coo(storage_t) # the flattened tensor\n",
    "print(flattened_coo.shape)\n",
    "flattened_coo_T = flattened_coo.T\n",
    "print(flattened_coo_T.shape)\n",
    "A = torch.sparse.mm(flattened_coo_T, flattened_coo)\n",
    "print(A.shape)\n",
    "print(A.type())\n",
    "# pdt = torch.matmul(flattened_coo_T, torch.arange(1000).view(-1, 1).to(dtype=torch.float32))\n",
    "# print(pdt.shape)\n",
    "# print(pdt.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.to_sparse_csr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [[0, 0, 1, 1], [0, 2, 1, 2]]\n",
    "# v = torch.randn(4 * 2 * 2).view((4, 2, 2)).to(dtype=torch.float32)\n",
    "v = torch.randn(4)\n",
    "sct = torch.sparse_coo_tensor(i, v)\n",
    "v2 = torch.randn(4)\n",
    "sct2 = torch.sparse_coo_tensor(i, v2)\n",
    "# torch.cat([sct, sct2], dim=1)\n",
    "# pdt = torch.sparse.mm(v, v2)\n",
    "pdt = sct @ sct2.T\n",
    "print(pdt.type())\n",
    "print(pdt.is_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the jacobian values from jacrev and vmap match\n",
    "batch_dim = jac_from_jacrev.shape[0]\n",
    "index = trimmed_dataset['camera_index_of_observations']\n",
    "jac_from_jacrev_condensed = jac_from_jacrev[torch.arange(batch_dim), index]\n",
    "print(jac_from_jacrev_condensed.shape)\n",
    "if torch.allclose(jac_from_vmap, jac_from_jacrev_condensed):\n",
    "  print(\"Jacobian structure sanity check: ok!\")\n",
    "else:\n",
    "  print(\"Jacobian structure sanity check: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually construct SBT from vmap result\n",
    "import torch\n",
    "import pypose as pp\n",
    "from pypose.sparse import sbktensor\n",
    "\n",
    "def construct_sbt(jac_from_vmap, num_cameras, camera_index):\n",
    "  # camera_index = torch.from_numpy(camera_index) # for torch.stack\n",
    "  n = camera_index.shape[0] # num 2D points\n",
    "  # i = torch.stack([torch.arange(n), camera_index, torch.zeros(n)])\n",
    "  i = torch.stack([torch.arange(n), camera_index])\n",
    "  print(i.shape)\n",
    "  # v = jac_from_vmap[:, None, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  v = jac_from_vmap[:, None, :] # adjust dimension to accomodate for sbt constructor\n",
    "  # return pp.sbktensor(i, v, size=(n, num_cameras, 1), dtype=torch.float32)\n",
    "  return pp.sbktensor.sbktensor(i, v, size=(n, num_cameras), dtype=torch.float32)\n",
    "\n",
    "sparse_jac = construct_sbt(jac_from_vmap, len(trimmed_dataset['camera_extrinsics']), trimmed_dataset['camera_index_of_observations'])\n",
    "dense_jac = sparse_jac.to_dense()\n",
    "# if torch.allclose(jac_from_jacrev, dense_jac):\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: ok!\")\n",
    "# else:\n",
    "#   print(\"Dense Jacobian <-> Sparse Jacobian: failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_t = sparse_jac._s\n",
    "print(f'shape: {storage_t.shape}, dense dim: {storage_t.dense_dim()}, \\\n",
    "sparse: {storage_t.sparse_dim()}')\n",
    "flattened_coo = pp.hybrid2coo(storage_t) # the flattened tensor\n",
    "print(flattened_coo.shape)\n",
    "flattened_coo_T = flattened_coo.T\n",
    "print(flattened_coo_T.shape)\n",
    "A = torch.sparse.mm(flattened_coo_T, flattened_coo)\n",
    "print(A.shape)\n",
    "print(A.type())\n",
    "# pdt = torch.matmul(flattened_coo_T, torch.arange(1000).view(-1, 1).to(dtype=torch.float32))\n",
    "# print(pdt.shape)\n",
    "# print(pdt.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_coo_diagonal(t: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    return t.values()[diag_indices]\n",
    "\n",
    "def sparse_coo_diagonal_clamp_(t: torch.Tensor, min_value, max_value):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].clamp_(min_value, max_value)\n",
    "\n",
    "\n",
    "def sparse_coo_diagonal_add_(t: torch.Tensor, other: torch.Tensor):\n",
    "    indices = t.indices()\n",
    "    diag_indices = indices[0] == indices[1]\n",
    "    t.values()[diag_indices] = t.values()[diag_indices].add_(other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_indices = (A.indices()[0] == A.indices()[1]).nonzero(as_tuple=True)\n",
    "# print(diag_indices[0][])\n",
    "sparse_coo_diagonal_clamp_(A, -100, 100)\n",
    "print(A.values()[diag_indices[0][:10]])\n",
    "sparse_coo_diagonal_add_(A, sparse_coo_diagonal(A) * torch.ones(301) * 2)\n",
    "print(A.values()[diag_indices[0][:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacrev_custom(func, argnums):\n",
    "  def wrapper(*args, **kwargs):\n",
    "    jac_vmap = torch.vmap(pp.func.jacrev(func)) # vmap\n",
    "    gradients = jac_vmap(*args)\n",
    "    sbt = construct_sbt(gradients, kwargs['num_cols'], kwargs['index'])\n",
    "    return sbt\n",
    "  return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_function_custom = jacrev_custom(reprojerr_vmap, argnums=0)\n",
    "jac_from_custom = jac_function_custom(trimmed_dataset['camera_extrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['points_3d'][trimmed_dataset['point_index_of_observations'], None],\n",
    "                      trimmed_dataset['points_2d'],\n",
    "                      trimmed_dataset['camera_intrinsics'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      trimmed_dataset['camera_distortions'][trimmed_dataset['camera_index_of_observations']],\n",
    "                      num_cols=len(trimmed_dataset['camera_extrinsics']),\n",
    "                      index=trimmed_dataset['camera_index_of_observations'])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pypose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
